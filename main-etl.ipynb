{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Installation des libairies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting findspark\n",
      "  Using cached findspark-2.0.0-py2.py3-none-any.whl (4.4 kB)\n",
      "Installing collected packages: findspark\n",
      "Successfully installed findspark-2.0.0\n",
      "Requirement already satisfied: pyspark in c:\\users\\antoine\\anaconda3\\lib\\site-packages (3.2.1)\n",
      "Requirement already satisfied: py4j==0.10.9.3 in c:\\users\\antoine\\anaconda3\\lib\\site-packages (from pyspark) (0.10.9.3)\n",
      "Requirement already satisfied: streamlit in c:\\users\\antoine\\anaconda3\\lib\\site-packages (1.5.0)\n",
      "Requirement already satisfied: tornado>=5.0 in c:\\users\\antoine\\anaconda3\\lib\\site-packages (from streamlit) (6.0.4)\n",
      "Requirement already satisfied: requests in c:\\users\\antoine\\anaconda3\\lib\\site-packages (from streamlit) (2.24.0)\n",
      "Requirement already satisfied: tzlocal in c:\\users\\antoine\\anaconda3\\lib\\site-packages (from streamlit) (4.1)\n",
      "Requirement already satisfied: altair>=3.2.0 in c:\\users\\antoine\\anaconda3\\lib\\site-packages (from streamlit) (4.2.0)\n",
      "Requirement already satisfied: blinker in c:\\users\\antoine\\anaconda3\\lib\\site-packages (from streamlit) (1.4)\n",
      "Requirement already satisfied: toml in c:\\users\\antoine\\anaconda3\\lib\\site-packages (from streamlit) (0.10.1)\n",
      "Requirement already satisfied: protobuf!=3.11,>=3.6.0 in c:\\users\\antoine\\anaconda3\\lib\\site-packages (from streamlit) (3.19.4)\n",
      "Requirement already satisfied: pympler>=0.9 in c:\\users\\antoine\\anaconda3\\lib\\site-packages (from streamlit) (1.0.1)\n",
      "Requirement already satisfied: click>=7.0 in c:\\users\\antoine\\anaconda3\\lib\\site-packages (from streamlit) (7.1.2)\n",
      "Requirement already satisfied: watchdog; platform_system != \"Darwin\" in c:\\users\\antoine\\anaconda3\\lib\\site-packages (from streamlit) (0.10.3)\n",
      "Requirement already satisfied: pydeck>=0.1.dev5 in c:\\users\\antoine\\anaconda3\\lib\\site-packages (from streamlit) (0.7.1)\n",
      "Requirement already satisfied: attrs in c:\\users\\antoine\\anaconda3\\lib\\site-packages (from streamlit) (19.3.0)\n",
      "Requirement already satisfied: numpy in c:\\users\\antoine\\anaconda3\\lib\\site-packages (from streamlit) (1.18.5)\n",
      "Requirement already satisfied: cachetools>=4.0 in c:\\users\\antoine\\anaconda3\\lib\\site-packages (from streamlit) (5.0.0)\n",
      "Requirement already satisfied: packaging in c:\\users\\antoine\\anaconda3\\lib\\site-packages (from streamlit) (20.4)\n",
      "Requirement already satisfied: base58 in c:\\users\\antoine\\anaconda3\\lib\\site-packages (from streamlit) (2.1.1)\n",
      "Requirement already satisfied: pandas>=0.21.0 in c:\\users\\antoine\\anaconda3\\lib\\site-packages (from streamlit) (1.0.5)\n",
      "Requirement already satisfied: astor in c:\\users\\antoine\\anaconda3\\lib\\site-packages (from streamlit) (0.8.1)\n",
      "Requirement already satisfied: pillow>=6.2.0 in c:\\users\\antoine\\anaconda3\\lib\\site-packages (from streamlit) (7.2.0)\n",
      "Requirement already satisfied: python-dateutil in c:\\users\\antoine\\anaconda3\\lib\\site-packages (from streamlit) (2.8.1)\n",
      "Requirement already satisfied: validators in c:\\users\\antoine\\anaconda3\\lib\\site-packages (from streamlit) (0.18.2)\n",
      "Requirement already satisfied: gitpython!=3.1.19 in c:\\users\\antoine\\anaconda3\\lib\\site-packages (from streamlit) (3.1.26)\n",
      "Requirement already satisfied: pyarrow in c:\\users\\antoine\\anaconda3\\lib\\site-packages (from streamlit) (7.0.0)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in c:\\users\\antoine\\anaconda3\\lib\\site-packages (from requests->streamlit) (2020.6.20)\n",
      "Requirement already satisfied: idna<3,>=2.5 in c:\\users\\antoine\\anaconda3\\lib\\site-packages (from requests->streamlit) (2.10)\n",
      "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in c:\\users\\antoine\\anaconda3\\lib\\site-packages (from requests->streamlit) (1.25.9)\n",
      "Requirement already satisfied: chardet<4,>=3.0.2 in c:\\users\\antoine\\anaconda3\\lib\\site-packages (from requests->streamlit) (3.0.4)\n",
      "Requirement already satisfied: pytz-deprecation-shim in c:\\users\\antoine\\anaconda3\\lib\\site-packages (from tzlocal->streamlit) (0.1.0.post0)\n",
      "Requirement already satisfied: backports.zoneinfo; python_version < \"3.9\" in c:\\users\\antoine\\anaconda3\\lib\\site-packages (from tzlocal->streamlit) (0.2.1)\n",
      "Requirement already satisfied: tzdata; platform_system == \"Windows\" in c:\\users\\antoine\\anaconda3\\lib\\site-packages (from tzlocal->streamlit) (2021.5)\n",
      "Requirement already satisfied: entrypoints in c:\\users\\antoine\\anaconda3\\lib\\site-packages (from altair>=3.2.0->streamlit) (0.3)\n",
      "Requirement already satisfied: jsonschema>=3.0 in c:\\users\\antoine\\anaconda3\\lib\\site-packages (from altair>=3.2.0->streamlit) (3.2.0)\n",
      "Requirement already satisfied: jinja2 in c:\\users\\antoine\\anaconda3\\lib\\site-packages (from altair>=3.2.0->streamlit) (2.11.2)\n",
      "Requirement already satisfied: toolz in c:\\users\\antoine\\anaconda3\\lib\\site-packages (from altair>=3.2.0->streamlit) (0.10.0)\n",
      "Requirement already satisfied: pathtools>=0.1.1 in c:\\users\\antoine\\anaconda3\\lib\\site-packages (from watchdog; platform_system != \"Darwin\"->streamlit) (0.1.2)\n",
      "Requirement already satisfied: ipywidgets>=7.0.0 in c:\\users\\antoine\\anaconda3\\lib\\site-packages (from pydeck>=0.1.dev5->streamlit) (7.5.1)\n",
      "Requirement already satisfied: traitlets>=4.3.2 in c:\\users\\antoine\\anaconda3\\lib\\site-packages (from pydeck>=0.1.dev5->streamlit) (4.3.3)\n",
      "Requirement already satisfied: ipykernel>=5.1.2; python_version >= \"3.4\" in c:\\users\\antoine\\anaconda3\\lib\\site-packages (from pydeck>=0.1.dev5->streamlit) (5.3.2)\n",
      "Requirement already satisfied: pyparsing>=2.0.2 in c:\\users\\antoine\\anaconda3\\lib\\site-packages (from packaging->streamlit) (2.4.7)\n",
      "Requirement already satisfied: six in c:\\users\\antoine\\anaconda3\\lib\\site-packages (from packaging->streamlit) (1.15.0)\n",
      "Requirement already satisfied: pytz>=2017.2 in c:\\users\\antoine\\anaconda3\\lib\\site-packages (from pandas>=0.21.0->streamlit) (2020.1)\n",
      "Requirement already satisfied: decorator>=3.4.0 in c:\\users\\antoine\\anaconda3\\lib\\site-packages (from validators->streamlit) (4.4.2)\n",
      "Requirement already satisfied: gitdb<5,>=4.0.1 in c:\\users\\antoine\\anaconda3\\lib\\site-packages (from gitpython!=3.1.19->streamlit) (4.0.9)\n",
      "Requirement already satisfied: setuptools in c:\\users\\antoine\\anaconda3\\lib\\site-packages (from jsonschema>=3.0->altair>=3.2.0->streamlit) (49.2.0.post20200714)\n",
      "Requirement already satisfied: pyrsistent>=0.14.0 in c:\\users\\antoine\\anaconda3\\lib\\site-packages (from jsonschema>=3.0->altair>=3.2.0->streamlit) (0.16.0)\n",
      "Requirement already satisfied: MarkupSafe>=0.23 in c:\\users\\antoine\\anaconda3\\lib\\site-packages (from jinja2->altair>=3.2.0->streamlit) (1.1.1)\n",
      "Requirement already satisfied: widgetsnbextension~=3.5.0 in c:\\users\\antoine\\anaconda3\\lib\\site-packages (from ipywidgets>=7.0.0->pydeck>=0.1.dev5->streamlit) (3.5.1)\n",
      "Requirement already satisfied: ipython>=4.0.0; python_version >= \"3.3\" in c:\\users\\antoine\\anaconda3\\lib\\site-packages (from ipywidgets>=7.0.0->pydeck>=0.1.dev5->streamlit) (7.31.1)\n",
      "Requirement already satisfied: nbformat>=4.2.0 in c:\\users\\antoine\\anaconda3\\lib\\site-packages (from ipywidgets>=7.0.0->pydeck>=0.1.dev5->streamlit) (5.0.7)\n",
      "Requirement already satisfied: ipython-genutils in c:\\users\\antoine\\anaconda3\\lib\\site-packages (from traitlets>=4.3.2->pydeck>=0.1.dev5->streamlit) (0.2.0)\n",
      "Requirement already satisfied: jupyter-client in c:\\users\\antoine\\anaconda3\\lib\\site-packages (from ipykernel>=5.1.2; python_version >= \"3.4\"->pydeck>=0.1.dev5->streamlit) (6.1.6)\n",
      "Requirement already satisfied: smmap<6,>=3.0.1 in c:\\users\\antoine\\anaconda3\\lib\\site-packages (from gitdb<5,>=4.0.1->gitpython!=3.1.19->streamlit) (5.0.0)\n",
      "Requirement already satisfied: notebook>=4.4.1 in c:\\users\\antoine\\anaconda3\\lib\\site-packages (from widgetsnbextension~=3.5.0->ipywidgets>=7.0.0->pydeck>=0.1.dev5->streamlit) (6.0.3)\n",
      "Requirement already satisfied: pygments in c:\\users\\antoine\\anaconda3\\lib\\site-packages (from ipython>=4.0.0; python_version >= \"3.3\"->ipywidgets>=7.0.0->pydeck>=0.1.dev5->streamlit) (2.6.1)\n",
      "Requirement already satisfied: pickleshare in c:\\users\\antoine\\anaconda3\\lib\\site-packages (from ipython>=4.0.0; python_version >= \"3.3\"->ipywidgets>=7.0.0->pydeck>=0.1.dev5->streamlit) (0.7.5)\n",
      "Requirement already satisfied: colorama; sys_platform == \"win32\" in c:\\users\\antoine\\anaconda3\\lib\\site-packages (from ipython>=4.0.0; python_version >= \"3.3\"->ipywidgets>=7.0.0->pydeck>=0.1.dev5->streamlit) (0.4.3)\n",
      "Requirement already satisfied: backcall in c:\\users\\antoine\\anaconda3\\lib\\site-packages (from ipython>=4.0.0; python_version >= \"3.3\"->ipywidgets>=7.0.0->pydeck>=0.1.dev5->streamlit) (0.2.0)\n",
      "Requirement already satisfied: jedi>=0.16 in c:\\users\\antoine\\anaconda3\\lib\\site-packages (from ipython>=4.0.0; python_version >= \"3.3\"->ipywidgets>=7.0.0->pydeck>=0.1.dev5->streamlit) (0.17.1)\n",
      "Requirement already satisfied: matplotlib-inline in c:\\users\\antoine\\anaconda3\\lib\\site-packages (from ipython>=4.0.0; python_version >= \"3.3\"->ipywidgets>=7.0.0->pydeck>=0.1.dev5->streamlit) (0.1.3)\n",
      "Requirement already satisfied: prompt-toolkit!=3.0.0,!=3.0.1,<3.1.0,>=2.0.0 in c:\\users\\antoine\\anaconda3\\lib\\site-packages (from ipython>=4.0.0; python_version >= \"3.3\"->ipywidgets>=7.0.0->pydeck>=0.1.dev5->streamlit) (3.0.5)\n",
      "Requirement already satisfied: jupyter-core in c:\\users\\antoine\\anaconda3\\lib\\site-packages (from nbformat>=4.2.0->ipywidgets>=7.0.0->pydeck>=0.1.dev5->streamlit) (4.6.3)\n",
      "Requirement already satisfied: pyzmq>=13 in c:\\users\\antoine\\anaconda3\\lib\\site-packages (from jupyter-client->ipykernel>=5.1.2; python_version >= \"3.4\"->pydeck>=0.1.dev5->streamlit) (19.0.1)\n",
      "Requirement already satisfied: terminado>=0.8.1 in c:\\users\\antoine\\anaconda3\\lib\\site-packages (from notebook>=4.4.1->widgetsnbextension~=3.5.0->ipywidgets>=7.0.0->pydeck>=0.1.dev5->streamlit) (0.8.3)\n",
      "Requirement already satisfied: prometheus-client in c:\\users\\antoine\\anaconda3\\lib\\site-packages (from notebook>=4.4.1->widgetsnbextension~=3.5.0->ipywidgets>=7.0.0->pydeck>=0.1.dev5->streamlit) (0.8.0)\n",
      "Requirement already satisfied: Send2Trash in c:\\users\\antoine\\anaconda3\\lib\\site-packages (from notebook>=4.4.1->widgetsnbextension~=3.5.0->ipywidgets>=7.0.0->pydeck>=0.1.dev5->streamlit) (1.5.0)\n",
      "Requirement already satisfied: nbconvert in c:\\users\\antoine\\anaconda3\\lib\\site-packages (from notebook>=4.4.1->widgetsnbextension~=3.5.0->ipywidgets>=7.0.0->pydeck>=0.1.dev5->streamlit) (5.6.1)\n",
      "Requirement already satisfied: parso<0.8.0,>=0.7.0 in c:\\users\\antoine\\anaconda3\\lib\\site-packages (from jedi>=0.16->ipython>=4.0.0; python_version >= \"3.3\"->ipywidgets>=7.0.0->pydeck>=0.1.dev5->streamlit) (0.7.0)\n",
      "Requirement already satisfied: wcwidth in c:\\users\\antoine\\anaconda3\\lib\\site-packages (from prompt-toolkit!=3.0.0,!=3.0.1,<3.1.0,>=2.0.0->ipython>=4.0.0; python_version >= \"3.3\"->ipywidgets>=7.0.0->pydeck>=0.1.dev5->streamlit) (0.2.5)\n",
      "Requirement already satisfied: pywin32>=1.0; sys_platform == \"win32\" in c:\\users\\antoine\\anaconda3\\lib\\site-packages (from jupyter-core->nbformat>=4.2.0->ipywidgets>=7.0.0->pydeck>=0.1.dev5->streamlit) (227)\n",
      "Requirement already satisfied: bleach in c:\\users\\antoine\\anaconda3\\lib\\site-packages (from nbconvert->notebook>=4.4.1->widgetsnbextension~=3.5.0->ipywidgets>=7.0.0->pydeck>=0.1.dev5->streamlit) (3.1.5)\n",
      "Requirement already satisfied: pandocfilters>=1.4.1 in c:\\users\\antoine\\anaconda3\\lib\\site-packages (from nbconvert->notebook>=4.4.1->widgetsnbextension~=3.5.0->ipywidgets>=7.0.0->pydeck>=0.1.dev5->streamlit) (1.4.2)\n",
      "Requirement already satisfied: testpath in c:\\users\\antoine\\anaconda3\\lib\\site-packages (from nbconvert->notebook>=4.4.1->widgetsnbextension~=3.5.0->ipywidgets>=7.0.0->pydeck>=0.1.dev5->streamlit) (0.4.4)\n",
      "Requirement already satisfied: mistune<2,>=0.8.1 in c:\\users\\antoine\\anaconda3\\lib\\site-packages (from nbconvert->notebook>=4.4.1->widgetsnbextension~=3.5.0->ipywidgets>=7.0.0->pydeck>=0.1.dev5->streamlit) (0.8.4)\n",
      "Requirement already satisfied: defusedxml in c:\\users\\antoine\\anaconda3\\lib\\site-packages (from nbconvert->notebook>=4.4.1->widgetsnbextension~=3.5.0->ipywidgets>=7.0.0->pydeck>=0.1.dev5->streamlit) (0.6.0)\n",
      "Requirement already satisfied: webencodings in c:\\users\\antoine\\anaconda3\\lib\\site-packages (from bleach->nbconvert->notebook>=4.4.1->widgetsnbextension~=3.5.0->ipywidgets>=7.0.0->pydeck>=0.1.dev5->streamlit) (0.5.1)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "ERROR: Could not find a version that satisfies the requirement nympy (from versions: none)\n",
      "ERROR: No matching distribution found for nympy\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting plotly\n",
      "  Using cached plotly-5.5.0-py2.py3-none-any.whl (26.5 MB)\n",
      "Requirement already satisfied: six in c:\\users\\antoine\\anaconda3\\lib\\site-packages (from plotly) (1.15.0)\n",
      "Collecting tenacity>=6.2.0\n",
      "  Downloading tenacity-8.0.1-py3-none-any.whl (24 kB)\n",
      "Installing collected packages: tenacity, plotly\n",
      "Successfully installed plotly-5.5.0 tenacity-8.0.1\n"
     ]
    }
   ],
   "source": [
    "#!pip install findspark\n",
    "#!pip install pyspark\n",
    "#!pip install streamlit\n",
    "#!pip install nympy\n",
    "#!pip install plotly"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Import des librairies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'C:\\\\Users\\\\Antoine\\\\anaconda3\\\\lib\\\\site-packages\\\\pyspark'"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import findspark\n",
    "findspark.init()\n",
    "findspark.find()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "from pyspark import SparkContext\n",
    "import pyspark.sql.functions as spf\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "#import streamlit as st\n",
    "import plotly as plt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Traitement ETL\n",
    "## Création d'un SparkSession"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "spark = SparkSession \\\n",
    "    .builder \\\n",
    "    .appName(\"how to read csv file\") \\\n",
    "    .getOrCreate()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Fonction d'ouverture de fichier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def openDataFrame(data):\n",
    "    df = spark.read.option(\"delimiter\", \",\") \\\n",
    "        .csv(f\"data/raw-file/{data}.csv\", header=True)\n",
    "    df.printSchema()\n",
    "    return df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Liste des fichiers à traiter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "fileName = [\"dre_auvergne_rhone_alpes\",\n",
    "            \"dre_bourgogne_franche_comte\",\n",
    "            \"dre_bretagne\",\n",
    "            \"dre_centre_val_de_loire\",\n",
    "            \"dre_corse\",\n",
    "            \"dre_france_metropolitaine\",\n",
    "            \"dre_grand_est\",\n",
    "            \"dre_hauts_de_france\",\n",
    "            \"dre_ile_de_france\",\n",
    "            \"dre_normandie\",\n",
    "            \"dre_nouvelle_aquitaine\",\n",
    "            \"dre_occitanie\",\n",
    "            \"dre_pays_de_la_loire\",\n",
    "            \"dre_provence_alpes_cote_d_azur\",\n",
    "           ]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Enlève la limitation de l'affichage du dataframe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "spark.conf.set('spark.sql.repl.eagerEval.enabled', True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Ouverture d'un fichier .csv et affichage de dataframe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- PRODUCTION: string (nullable = true)\n",
      " |-- _c1: string (nullable = true)\n",
      " |-- Unité: string (nullable = true)\n",
      " |-- 2014: string (nullable = true)\n",
      " |-- 2015: string (nullable = true)\n",
      " |-- 2016: string (nullable = true)\n",
      " |-- 2017: string (nullable = true)\n",
      " |-- 2018: string (nullable = true)\n",
      " |-- 2019: string (nullable = true)\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<table border='1'>\n",
       "<tr><th>PRODUCTION</th><th>_c1</th><th>Unité</th><th>2014</th><th>2015</th><th>2016</th><th>2017</th><th>2018</th><th>2019</th></tr>\n",
       "<tr><td>Production d&#x27;éner...</td><td>null</td><td>null</td><td>null</td><td>null</td><td>null</td><td>null</td><td>null</td><td>null</td></tr>\n",
       "<tr><td>P1</td><td>Production de cha...</td><td>kt</td><td>0</td><td>0</td><td>0</td><td>0</td><td>0</td><td>0</td></tr>\n",
       "<tr><td>P2</td><td>Production de pét...</td><td>kt</td><td>0</td><td>0</td><td>0</td><td>0</td><td>0</td><td>0</td></tr>\n",
       "<tr><td>P3</td><td>Production de gaz...</td><td>GWh PCS</td><td>0</td><td>0</td><td>0</td><td>0</td><td>0</td><td>0</td></tr>\n",
       "<tr><td>P4</td><td>Chaleur nucléaire...</td><td>GWh</td><td>266302</td><td>275368</td><td>227294</td><td>242398</td><td>243029</td><td>260090</td></tr>\n",
       "<tr><td>P5</td><td>Énergies renouvel...</td><td>GWh</td><td>30532</td><td>28268</td><td>30385</td><td>25659</td><td>32400</td><td>29961</td></tr>\n",
       "<tr><td>P7</td><td>Production totale...</td><td>ktep</td><td>0</td><td>0</td><td>0</td><td>0</td><td>0</td><td>0</td></tr>\n",
       "<tr><td>P8</td><td>  - production de...</td><td>ktep</td><td>0</td><td>0</td><td>0</td><td>0</td><td>0</td><td>0</td></tr>\n",
       "<tr><td>P9</td><td>  - production de...</td><td>ktep</td><td>0</td><td>0</td><td>0</td><td>0</td><td>0</td><td>0</td></tr>\n",
       "<tr><td>P10</td><td>  - production de...</td><td>ktep</td><td>0</td><td>0</td><td>0</td><td>0</td><td>0</td><td>0</td></tr>\n",
       "<tr><td>P11</td><td>Chaleur nucléaire...</td><td>ktep</td><td>22898</td><td>23677</td><td>19544</td><td>20842</td><td>20897</td><td>22364</td></tr>\n",
       "<tr><td>P12</td><td>Énergies renouvel...</td><td>ktep</td><td>2625</td><td>2431</td><td>2613</td><td>2206</td><td>2786</td><td>2576</td></tr>\n",
       "<tr><td>P13</td><td>Biométhane injecté</td><td>GWh PCI</td><td>0</td><td>1</td><td>7</td><td>32</td><td>41</td><td>61</td></tr>\n",
       "<tr><td>P14</td><td>Biométhane inject...</td><td>ktep</td><td>0</td><td>0</td><td>1</td><td>3</td><td>4</td><td>5</td></tr>\n",
       "<tr><td>Électricité</td><td>null</td><td>null</td><td>null</td><td>null</td><td>null</td><td>null</td><td>null</td><td>null</td></tr>\n",
       "<tr><td>Productions</td><td>null</td><td>null</td><td>null</td><td>null</td><td>null</td><td>null</td><td>null</td><td>null</td></tr>\n",
       "<tr><td>E1</td><td>Production totale...</td><td>GWh</td><td>120289</td><td>121896</td><td>108897</td><td>108896</td><td>115223</td><td>119056</td></tr>\n",
       "<tr><td>E2</td><td>Production d&#x27;élec...</td><td>GWh</td><td>30532</td><td>28268</td><td>30385</td><td>25659</td><td>32400</td><td>29961</td></tr>\n",
       "<tr><td>E4</td><td>  - Hydraulique (...</td><td>GWh</td><td>29 120</td><td>26 706</td><td>28 708</td><td>23 769</td><td>30 280</td><td>27 535</td></tr>\n",
       "<tr><td>E6</td><td>  - Éolien </td><td>GWh</td><td>784</td><td>803</td><td>872</td><td>1 006</td><td>1 075</td><td>1 194</td></tr>\n",
       "</table>\n",
       "only showing top 20 rows\n"
      ],
      "text/plain": [
       "+--------------------+--------------------+-------+------+------+------+------+------+------+\n",
       "|          PRODUCTION|                 _c1|  Unité|  2014|  2015|  2016|  2017|  2018|  2019|\n",
       "+--------------------+--------------------+-------+------+------+------+------+------+------+\n",
       "|Production d'éner...|                null|   null|  null|  null|  null|  null|  null|  null|\n",
       "|                  P1|Production de cha...|     kt|     0|     0|     0|     0|     0|     0|\n",
       "|                  P2|Production de pét...|     kt|     0|     0|     0|     0|     0|     0|\n",
       "|                  P3|Production de gaz...|GWh PCS|     0|     0|     0|     0|     0|     0|\n",
       "|                  P4|Chaleur nucléaire...|    GWh|266302|275368|227294|242398|243029|260090|\n",
       "|                  P5|Énergies renouvel...|    GWh| 30532| 28268| 30385| 25659| 32400| 29961|\n",
       "|                  P7|Production totale...|   ktep|     0|     0|     0|     0|     0|     0|\n",
       "|                  P8|  - production de...|   ktep|     0|     0|     0|     0|     0|     0|\n",
       "|                  P9|  - production de...|   ktep|     0|     0|     0|     0|     0|     0|\n",
       "|                 P10|  - production de...|   ktep|     0|     0|     0|     0|     0|     0|\n",
       "|                 P11|Chaleur nucléaire...|   ktep| 22898| 23677| 19544| 20842| 20897| 22364|\n",
       "|                 P12|Énergies renouvel...|   ktep|  2625|  2431|  2613|  2206|  2786|  2576|\n",
       "|                 P13|  Biométhane injecté|GWh PCI|     0|     1|     7|    32|    41|    61|\n",
       "|                 P14|Biométhane inject...|   ktep|     0|     0|     1|     3|     4|     5|\n",
       "|         Électricité|                null|   null|  null|  null|  null|  null|  null|  null|\n",
       "|         Productions|                null|   null|  null|  null|  null|  null|  null|  null|\n",
       "|                  E1|Production totale...|    GWh|120289|121896|108897|108896|115223|119056|\n",
       "|                  E2|Production d'élec...|    GWh| 30532| 28268| 30385| 25659| 32400| 29961|\n",
       "|                  E4|  - Hydraulique (...|    GWh|29 120|26 706|28 708|23 769|30 280|27 535|\n",
       "|                  E6|           - Éolien |    GWh|   784|   803|   872| 1 006| 1 075| 1 194|\n",
       "+--------------------+--------------------+-------+------+------+------+------+------+------+\n",
       "only showing top 20 rows"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = openDataFrame(fileName[0])\n",
    "df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Transposition de l'affichage"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-RECORD 0--------------------------\n",
      " PRODUCTION | Production d'éner... \n",
      " _c1        | null                 \n",
      " Unité      | null                 \n",
      " 2014       | null                 \n",
      " 2015       | null                 \n",
      " 2016       | null                 \n",
      " 2017       | null                 \n",
      " 2018       | null                 \n",
      " 2019       | null                 \n",
      "-RECORD 1--------------------------\n",
      " PRODUCTION | P1                   \n",
      " _c1        | Production de cha... \n",
      " Unité      | kt                   \n",
      " 2014       | 0                    \n",
      " 2015       | 0                    \n",
      " 2016       | 0                    \n",
      " 2017       | 0                    \n",
      " 2018       | 0                    \n",
      " 2019       | 0                    \n",
      "-RECORD 2--------------------------\n",
      " PRODUCTION | P2                   \n",
      " _c1        | Production de pét... \n",
      " Unité      | kt                   \n",
      " 2014       | 0                    \n",
      " 2015       | 0                    \n",
      " 2016       | 0                    \n",
      " 2017       | 0                    \n",
      " 2018       | 0                    \n",
      " 2019       | 0                    \n",
      "only showing top 3 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.show(3, vertical=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Affichage des colonnes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['PRODUCTION', '_c1', 'Unité', '2014', '2015', '2016', '2017', '2018', '2019']"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.columns"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Affichage des données"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- PRODUCTION: string (nullable = true)\n",
      " |-- _c1: string (nullable = true)\n",
      " |-- Unité: string (nullable = true)\n",
      " |-- 2014: string (nullable = true)\n",
      " |-- 2015: string (nullable = true)\n",
      " |-- 2016: string (nullable = true)\n",
      " |-- 2017: string (nullable = true)\n",
      " |-- 2018: string (nullable = true)\n",
      " |-- 2019: string (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.printSchema()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "La deuxième colonne ne porte pas de nom ..\n",
    "Nous allons y remèdier maintenant 😉"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<table border='1'>\n",
       "<tr><th>PRODUCTION</th><th>Intitulé énergie</th><th>Unité</th><th>2014</th><th>2015</th><th>2016</th><th>2017</th><th>2018</th><th>2019</th></tr>\n",
       "<tr><td>Production d&#x27;éner...</td><td>null</td><td>null</td><td>null</td><td>null</td><td>null</td><td>null</td><td>null</td><td>null</td></tr>\n",
       "<tr><td>P1</td><td>Production de cha...</td><td>kt</td><td>0</td><td>0</td><td>0</td><td>0</td><td>0</td><td>0</td></tr>\n",
       "<tr><td>P2</td><td>Production de pét...</td><td>kt</td><td>0</td><td>0</td><td>0</td><td>0</td><td>0</td><td>0</td></tr>\n",
       "<tr><td>P3</td><td>Production de gaz...</td><td>GWh PCS</td><td>0</td><td>0</td><td>0</td><td>0</td><td>0</td><td>0</td></tr>\n",
       "<tr><td>P4</td><td>Chaleur nucléaire...</td><td>GWh</td><td>266302</td><td>275368</td><td>227294</td><td>242398</td><td>243029</td><td>260090</td></tr>\n",
       "<tr><td>P5</td><td>Énergies renouvel...</td><td>GWh</td><td>30532</td><td>28268</td><td>30385</td><td>25659</td><td>32400</td><td>29961</td></tr>\n",
       "<tr><td>P7</td><td>Production totale...</td><td>ktep</td><td>0</td><td>0</td><td>0</td><td>0</td><td>0</td><td>0</td></tr>\n",
       "<tr><td>P8</td><td>  - production de...</td><td>ktep</td><td>0</td><td>0</td><td>0</td><td>0</td><td>0</td><td>0</td></tr>\n",
       "<tr><td>P9</td><td>  - production de...</td><td>ktep</td><td>0</td><td>0</td><td>0</td><td>0</td><td>0</td><td>0</td></tr>\n",
       "<tr><td>P10</td><td>  - production de...</td><td>ktep</td><td>0</td><td>0</td><td>0</td><td>0</td><td>0</td><td>0</td></tr>\n",
       "<tr><td>P11</td><td>Chaleur nucléaire...</td><td>ktep</td><td>22898</td><td>23677</td><td>19544</td><td>20842</td><td>20897</td><td>22364</td></tr>\n",
       "<tr><td>P12</td><td>Énergies renouvel...</td><td>ktep</td><td>2625</td><td>2431</td><td>2613</td><td>2206</td><td>2786</td><td>2576</td></tr>\n",
       "<tr><td>P13</td><td>Biométhane injecté</td><td>GWh PCI</td><td>0</td><td>1</td><td>7</td><td>32</td><td>41</td><td>61</td></tr>\n",
       "<tr><td>P14</td><td>Biométhane inject...</td><td>ktep</td><td>0</td><td>0</td><td>1</td><td>3</td><td>4</td><td>5</td></tr>\n",
       "<tr><td>Électricité</td><td>null</td><td>null</td><td>null</td><td>null</td><td>null</td><td>null</td><td>null</td><td>null</td></tr>\n",
       "<tr><td>Productions</td><td>null</td><td>null</td><td>null</td><td>null</td><td>null</td><td>null</td><td>null</td><td>null</td></tr>\n",
       "<tr><td>E1</td><td>Production totale...</td><td>GWh</td><td>120289</td><td>121896</td><td>108897</td><td>108896</td><td>115223</td><td>119056</td></tr>\n",
       "<tr><td>E2</td><td>Production d&#x27;élec...</td><td>GWh</td><td>30532</td><td>28268</td><td>30385</td><td>25659</td><td>32400</td><td>29961</td></tr>\n",
       "<tr><td>E4</td><td>  - Hydraulique (...</td><td>GWh</td><td>29 120</td><td>26 706</td><td>28 708</td><td>23 769</td><td>30 280</td><td>27 535</td></tr>\n",
       "<tr><td>E6</td><td>  - Éolien </td><td>GWh</td><td>784</td><td>803</td><td>872</td><td>1 006</td><td>1 075</td><td>1 194</td></tr>\n",
       "</table>\n",
       "only showing top 20 rows\n"
      ],
      "text/plain": [
       "+--------------------+--------------------+-------+------+------+------+------+------+------+\n",
       "|          PRODUCTION|    Intitulé énergie|  Unité|  2014|  2015|  2016|  2017|  2018|  2019|\n",
       "+--------------------+--------------------+-------+------+------+------+------+------+------+\n",
       "|Production d'éner...|                null|   null|  null|  null|  null|  null|  null|  null|\n",
       "|                  P1|Production de cha...|     kt|     0|     0|     0|     0|     0|     0|\n",
       "|                  P2|Production de pét...|     kt|     0|     0|     0|     0|     0|     0|\n",
       "|                  P3|Production de gaz...|GWh PCS|     0|     0|     0|     0|     0|     0|\n",
       "|                  P4|Chaleur nucléaire...|    GWh|266302|275368|227294|242398|243029|260090|\n",
       "|                  P5|Énergies renouvel...|    GWh| 30532| 28268| 30385| 25659| 32400| 29961|\n",
       "|                  P7|Production totale...|   ktep|     0|     0|     0|     0|     0|     0|\n",
       "|                  P8|  - production de...|   ktep|     0|     0|     0|     0|     0|     0|\n",
       "|                  P9|  - production de...|   ktep|     0|     0|     0|     0|     0|     0|\n",
       "|                 P10|  - production de...|   ktep|     0|     0|     0|     0|     0|     0|\n",
       "|                 P11|Chaleur nucléaire...|   ktep| 22898| 23677| 19544| 20842| 20897| 22364|\n",
       "|                 P12|Énergies renouvel...|   ktep|  2625|  2431|  2613|  2206|  2786|  2576|\n",
       "|                 P13|  Biométhane injecté|GWh PCI|     0|     1|     7|    32|    41|    61|\n",
       "|                 P14|Biométhane inject...|   ktep|     0|     0|     1|     3|     4|     5|\n",
       "|         Électricité|                null|   null|  null|  null|  null|  null|  null|  null|\n",
       "|         Productions|                null|   null|  null|  null|  null|  null|  null|  null|\n",
       "|                  E1|Production totale...|    GWh|120289|121896|108897|108896|115223|119056|\n",
       "|                  E2|Production d'élec...|    GWh| 30532| 28268| 30385| 25659| 32400| 29961|\n",
       "|                  E4|  - Hydraulique (...|    GWh|29 120|26 706|28 708|23 769|30 280|27 535|\n",
       "|                  E6|           - Éolien |    GWh|   784|   803|   872| 1 006| 1 075| 1 194|\n",
       "+--------------------+--------------------+-------+------+------+------+------+------+------+\n",
       "only showing top 20 rows"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# def withColumnRenamed(existingName: String, newName: String): DataFrame\n",
    "df = df.withColumnRenamed(\"_c1\",\"Intitulé énergie\")\n",
    "df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Sommaire des données"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+-----------------+-----------------+\n",
      "|summary|             2014|             2015|\n",
      "+-------+-----------------+-----------------+\n",
      "|  count|              123|              123|\n",
      "|   mean|7200.949367088608|7521.220779220779|\n",
      "| stddev|32936.38346909151|34321.52734326226|\n",
      "|    min|                0|                0|\n",
      "|    max|  Secret primaire|  Secret primaire|\n",
      "+-------+-----------------+-----------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.select('2014', '2015').describe().show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Permière version des liste\n",
    "\n",
    "list_production = [\n",
    "    'P1','P2','P3','P4','P5','P7','P8','P9','P10','P11','P12','P13','P14',\n",
    "    'E1','E2','E4','E6','E7','E8','E9','E10','E11','E12','E13','E14','E15','E16','E17','E18','E19','E20','E21','E22','E23','E24','E25','E26','E27','E28'\n",
    "]\n",
    "\n",
    "list_consomation = [\n",
    "    'C1','CI1','CI3','CI3b','CI4','CI9','CI11','CI5','CI6','CI7','CI8',\n",
    "    'CTR1','CTR2','CTR3','CTR4','CTR5','CTR6','CTR7','CTR8','CTR9','CTR10','CTR11',\n",
    "    'CR1','CR7','CR2','CR3','CR4','CR5','CR6','CR8',\n",
    "    'CTE1','CTE7','CTE2','CTE3','CTE4','CTE5','CTE6','CTE8',\n",
    "    'CA1','CA7','CA2','CA21','CA3','CA4','CA5','CA6','CA8',\n",
    "    'C2','C3','C4','C13','C14'\n",
    "]\n",
    "\n",
    "# Penser à passer par une tranposition puis pour un passae d'indexe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+--------------------+-------+------+------+------+------+------+------+\n",
      "|PRODUCTION|    Intitulé énergie|  Unité|  2014|  2015|  2016|  2017|  2018|  2019|\n",
      "+----------+--------------------+-------+------+------+------+------+------+------+\n",
      "|        P1|Production de cha...|     kt|     0|     0|     0|     0|     0|     0|\n",
      "|        P2|Production de pét...|     kt|     0|     0|     0|     0|     0|     0|\n",
      "|        P3|Production de gaz...|GWh PCS|     0|     0|     0|     0|     0|     0|\n",
      "|        P4|Chaleur nucléaire...|    GWh|266302|275368|227294|242398|243029|260090|\n",
      "|        P5|Énergies renouvel...|    GWh| 30532| 28268| 30385| 25659| 32400| 29961|\n",
      "|        P7|Production totale...|   ktep|     0|     0|     0|     0|     0|     0|\n",
      "|        P8|  - production de...|   ktep|     0|     0|     0|     0|     0|     0|\n",
      "|        P9|  - production de...|   ktep|     0|     0|     0|     0|     0|     0|\n",
      "|       P10|  - production de...|   ktep|     0|     0|     0|     0|     0|     0|\n",
      "|       P11|Chaleur nucléaire...|   ktep| 22898| 23677| 19544| 20842| 20897| 22364|\n",
      "|       P12|Énergies renouvel...|   ktep|  2625|  2431|  2613|  2206|  2786|  2576|\n",
      "|       P13|  Biométhane injecté|GWh PCI|     0|     1|     7|    32|    41|    61|\n",
      "|       P14|Biométhane inject...|   ktep|     0|     0|     1|     3|     4|     5|\n",
      "|        E1|Production totale...|    GWh|120289|121896|108897|108896|115223|119056|\n",
      "|        E2|Production d'élec...|    GWh| 30532| 28268| 30385| 25659| 32400| 29961|\n",
      "|        E4|  - Hydraulique (...|    GWh|29 120|26 706|28 708|23 769|30 280|27 535|\n",
      "|        E6|           - Éolien |    GWh|   784|   803|   872| 1 006| 1 075| 1 194|\n",
      "|        E7|  - Solaire photo...|    GWh|   627|   759|   805|   883| 1 045| 1 232|\n",
      "|        E8|Production d'élec...|    GWh|87 880|90 871|75 007|79 991|80 199|85 830|\n",
      "|        E9|Production nette ...|    GWh|  1877|  2756|  3506|  3247|  2624|  3265|\n",
      "+----------+--------------------+-------+------+------+------+------+------+------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df_production = df.filter(df.PRODUCTION.isin(list_production))\n",
    "df_production.show()\n",
    "\n",
    "## Liste de test non concluant\n",
    "#subset_df = df.filter(df.PRODUCTION.isin(list_production))\n",
    "#df_production = spark.createDataFrame(subset_df, df.columns)\n",
    "#df_production = df.withColumn(df.columns[0], spf.when(df.PRODUCTION.isin(list_production)))\n",
    "#df_production.show"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+--------------------+-------+------+------+------+------+------+------+\n",
      "|PRODUCTION|    Intitulé énergie|  Unité|  2014|  2015|  2016|  2017|  2018|  2019|\n",
      "+----------+--------------------+-------+------+------+------+------+------+------+\n",
      "|        C1|Consommation fina...|   ktep| 17948| 18567| 18559| 18746| 18327| 18369|\n",
      "|       CI1|           Industrie|   ktep|  3802|  3860|  3971|  3930|  3826|  3909|\n",
      "|       CI3|             Charbon|   ktep|    60|    61|    63|   112|   114|   112|\n",
      "|      CI3b|dont hauts fourneaux|   ktep|     0|     0|     0|     0|     0|     0|\n",
      "|       CI4| Produits pétroliers|   ktep|   387|   356|   366|   362|   347|   350|\n",
      "|       CI9|Energies renouvel...|   ktep|   156|   154|   164|   148|   160|   169|\n",
      "|      CI11|Chaleur commercia...|   ktep|   215|   236|   231|   222|   181|   258|\n",
      "|       CI5|Gaz naturel (unit...|GWh PCS|16 969|17 484|18 511|17 338|16 490|16 819|\n",
      "|       CI6|         Gaz naturel|   ktep| 1 313| 1 353| 1 432| 1 342| 1 276| 1 302|\n",
      "|       CI7|Électricité (unit...|    GWh|19 446|19 767|19 946|20 280|20 337|19 971|\n",
      "|       CI8|        Électricité |   ktep| 1 672| 1 700| 1 715| 1 744| 1 749| 1 717|\n",
      "|      CTR1|           Transport|   ktep|  6282|  6414|  6288|  6408|  6359|  6382|\n",
      "|      CTR2|Produits pétrolie...|   ktep| 5 765| 5 883| 5 769| 5 864| 5 815| 5 821|\n",
      "|      CTR3|       Biocarburants|   ktep|   400|   409|   400|   423|   427|   438|\n",
      "|      CTR4|Supercarburants (...|   ktep|   936|   972| 1 000| 1 049| 1 101| 1 190|\n",
      "|      CTR5|Gazole routier (l...|   ktep| 4 704| 4 782| 4 656| 4 697| 4 572| 4 516|\n",
      "|      CTR6|Biocarburants ess...|   ktep|    53|    57|    62|    71|    78|    88|\n",
      "|      CTR7|Biocarburants gazole|   ktep|   347|   352|   337|   350|   347|   348|\n",
      "|      CTR8|Gaz naturel (unit...|GWh PCS|    73|    75|    77|    84|   127|   162|\n",
      "|      CTR9|         Gaz naturel|   ktep|     6|     6|     6|     6|    10|    13|\n",
      "+----------+--------------------+-------+------+------+------+------+------+------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df_consomation = df.filter(df.PRODUCTION.isin(list_consomation))\n",
    "df_consomation.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Sauvegardes \n",
    "\n",
    "###  /!\\ En Chantier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "def saveDataFrame(dataframe, file_name):\n",
    "    file = f\"data/refined-file/{file_name}.csv\"\n",
    "    dataframe.write.format('csv').option('header',True).mode('overwrite').option('sep',',').save(file)\n",
    "    return f\"{file_name} as been succesfully create\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "def i(dataframe):\n",
    "    dataframe.write.format('jdbc').options(\n",
    "      url='jdbc:mysql://localhost/spark_nrg',\n",
    "      driver='com.mysql.jdbc.Driver',\n",
    "      dbtable='D_TEMP',\n",
    "      user='user',\n",
    "      password='user').mode('append').save()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "file = f\"production_{fileName[0]}\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "ename": "Py4JJavaError",
     "evalue": "An error occurred while calling o204.save.\n: java.lang.ClassNotFoundException: com.mysql.jdbc.Driver\r\n\tat java.net.URLClassLoader.findClass(Unknown Source)\r\n\tat java.lang.ClassLoader.loadClass(Unknown Source)\r\n\tat java.lang.ClassLoader.loadClass(Unknown Source)\r\n\tat org.apache.spark.sql.execution.datasources.jdbc.DriverRegistry$.register(DriverRegistry.scala:46)\r\n\tat org.apache.spark.sql.execution.datasources.jdbc.JDBCOptions.$anonfun$driverClass$1(JDBCOptions.scala:101)\r\n\tat org.apache.spark.sql.execution.datasources.jdbc.JDBCOptions.$anonfun$driverClass$1$adapted(JDBCOptions.scala:101)\r\n\tat scala.Option.foreach(Option.scala:407)\r\n\tat org.apache.spark.sql.execution.datasources.jdbc.JDBCOptions.<init>(JDBCOptions.scala:101)\r\n\tat org.apache.spark.sql.execution.datasources.jdbc.JdbcOptionsInWrite.<init>(JDBCOptions.scala:218)\r\n\tat org.apache.spark.sql.execution.datasources.jdbc.JdbcOptionsInWrite.<init>(JDBCOptions.scala:222)\r\n\tat org.apache.spark.sql.execution.datasources.jdbc.JdbcRelationProvider.createRelation(JdbcRelationProvider.scala:46)\r\n\tat org.apache.spark.sql.execution.datasources.SaveIntoDataSourceCommand.run(SaveIntoDataSourceCommand.scala:45)\r\n\tat org.apache.spark.sql.execution.command.ExecutedCommandExec.sideEffectResult$lzycompute(commands.scala:75)\r\n\tat org.apache.spark.sql.execution.command.ExecutedCommandExec.sideEffectResult(commands.scala:73)\r\n\tat org.apache.spark.sql.execution.command.ExecutedCommandExec.executeCollect(commands.scala:84)\r\n\tat org.apache.spark.sql.execution.QueryExecution$$anonfun$eagerlyExecuteCommands$1.$anonfun$applyOrElse$1(QueryExecution.scala:110)\r\n\tat org.apache.spark.sql.execution.SQLExecution$.$anonfun$withNewExecutionId$5(SQLExecution.scala:103)\r\n\tat org.apache.spark.sql.execution.SQLExecution$.withSQLConfPropagated(SQLExecution.scala:163)\r\n\tat org.apache.spark.sql.execution.SQLExecution$.$anonfun$withNewExecutionId$1(SQLExecution.scala:90)\r\n\tat org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:775)\r\n\tat org.apache.spark.sql.execution.SQLExecution$.withNewExecutionId(SQLExecution.scala:64)\r\n\tat org.apache.spark.sql.execution.QueryExecution$$anonfun$eagerlyExecuteCommands$1.applyOrElse(QueryExecution.scala:110)\r\n\tat org.apache.spark.sql.execution.QueryExecution$$anonfun$eagerlyExecuteCommands$1.applyOrElse(QueryExecution.scala:106)\r\n\tat org.apache.spark.sql.catalyst.trees.TreeNode.$anonfun$transformDownWithPruning$1(TreeNode.scala:481)\r\n\tat org.apache.spark.sql.catalyst.trees.CurrentOrigin$.withOrigin(TreeNode.scala:82)\r\n\tat org.apache.spark.sql.catalyst.trees.TreeNode.transformDownWithPruning(TreeNode.scala:481)\r\n\tat org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.org$apache$spark$sql$catalyst$plans$logical$AnalysisHelper$$super$transformDownWithPruning(LogicalPlan.scala:30)\r\n\tat org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.transformDownWithPruning(AnalysisHelper.scala:267)\r\n\tat org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.transformDownWithPruning$(AnalysisHelper.scala:263)\r\n\tat org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.transformDownWithPruning(LogicalPlan.scala:30)\r\n\tat org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.transformDownWithPruning(LogicalPlan.scala:30)\r\n\tat org.apache.spark.sql.catalyst.trees.TreeNode.transformDown(TreeNode.scala:457)\r\n\tat org.apache.spark.sql.execution.QueryExecution.eagerlyExecuteCommands(QueryExecution.scala:106)\r\n\tat org.apache.spark.sql.execution.QueryExecution.commandExecuted$lzycompute(QueryExecution.scala:93)\r\n\tat org.apache.spark.sql.execution.QueryExecution.commandExecuted(QueryExecution.scala:91)\r\n\tat org.apache.spark.sql.execution.QueryExecution.assertCommandExecuted(QueryExecution.scala:128)\r\n\tat org.apache.spark.sql.DataFrameWriter.runCommand(DataFrameWriter.scala:848)\r\n\tat org.apache.spark.sql.DataFrameWriter.saveToV1Source(DataFrameWriter.scala:382)\r\n\tat org.apache.spark.sql.DataFrameWriter.saveInternal(DataFrameWriter.scala:355)\r\n\tat org.apache.spark.sql.DataFrameWriter.save(DataFrameWriter.scala:247)\r\n\tat sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)\r\n\tat sun.reflect.NativeMethodAccessorImpl.invoke(Unknown Source)\r\n\tat sun.reflect.DelegatingMethodAccessorImpl.invoke(Unknown Source)\r\n\tat java.lang.reflect.Method.invoke(Unknown Source)\r\n\tat py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244)\r\n\tat py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:357)\r\n\tat py4j.Gateway.invoke(Gateway.java:282)\r\n\tat py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132)\r\n\tat py4j.commands.CallCommand.execute(CallCommand.java:79)\r\n\tat py4j.ClientServerConnection.waitForCommands(ClientServerConnection.java:182)\r\n\tat py4j.ClientServerConnection.run(ClientServerConnection.java:106)\r\n\tat java.lang.Thread.run(Unknown Source)\r\n",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mPy4JJavaError\u001b[0m                             Traceback (most recent call last)",
      "\u001b[1;32mC:\\Users\\DEADEL~1\\AppData\\Local\\Temp/ipykernel_20300/127941854.py\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[0mtest\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdf_production\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;32mC:\\Users\\DEADEL~1\\AppData\\Local\\Temp/ipykernel_20300/3697963215.py\u001b[0m in \u001b[0;36mtest\u001b[1;34m(dataframe)\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[1;32mdef\u001b[0m \u001b[0mtest\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdataframe\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 2\u001b[1;33m     dataframe.write.format('jdbc').options(\n\u001b[0m\u001b[0;32m      3\u001b[0m       \u001b[0murl\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;34m'jdbc:mysql://localhost/spark_nrg'\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      4\u001b[0m       \u001b[0mdriver\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;34m'com.mysql.jdbc.Driver'\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      5\u001b[0m       \u001b[0mdbtable\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;34m'D_TEMP'\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\spark-3.2.1-bin-hadoop3.2\\python\\pyspark\\sql\\readwriter.py\u001b[0m in \u001b[0;36msave\u001b[1;34m(self, path, format, mode, partitionBy, **options)\u001b[0m\n\u001b[0;32m    736\u001b[0m             \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mformat\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mformat\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    737\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mpath\u001b[0m \u001b[1;32mis\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 738\u001b[1;33m             \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_jwrite\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msave\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    739\u001b[0m         \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    740\u001b[0m             \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_jwrite\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msave\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mpath\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\spark-3.2.1-bin-hadoop3.2\\python\\lib\\py4j-0.10.9.3-src.zip\\py4j\\java_gateway.py\u001b[0m in \u001b[0;36m__call__\u001b[1;34m(self, *args)\u001b[0m\n\u001b[0;32m   1319\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1320\u001b[0m         \u001b[0manswer\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mgateway_client\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msend_command\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mcommand\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1321\u001b[1;33m         return_value = get_return_value(\n\u001b[0m\u001b[0;32m   1322\u001b[0m             answer, self.gateway_client, self.target_id, self.name)\n\u001b[0;32m   1323\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\spark-3.2.1-bin-hadoop3.2\\python\\pyspark\\sql\\utils.py\u001b[0m in \u001b[0;36mdeco\u001b[1;34m(*a, **kw)\u001b[0m\n\u001b[0;32m    109\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0mdeco\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0ma\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkw\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    110\u001b[0m         \u001b[1;32mtry\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 111\u001b[1;33m             \u001b[1;32mreturn\u001b[0m \u001b[0mf\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0ma\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkw\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    112\u001b[0m         \u001b[1;32mexcept\u001b[0m \u001b[0mpy4j\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mprotocol\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mPy4JJavaError\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    113\u001b[0m             \u001b[0mconverted\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mconvert_exception\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0me\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mjava_exception\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\spark-3.2.1-bin-hadoop3.2\\python\\lib\\py4j-0.10.9.3-src.zip\\py4j\\protocol.py\u001b[0m in \u001b[0;36mget_return_value\u001b[1;34m(answer, gateway_client, target_id, name)\u001b[0m\n\u001b[0;32m    324\u001b[0m             \u001b[0mvalue\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mOUTPUT_CONVERTER\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mtype\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0manswer\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m2\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mgateway_client\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    325\u001b[0m             \u001b[1;32mif\u001b[0m \u001b[0manswer\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m==\u001b[0m \u001b[0mREFERENCE_TYPE\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 326\u001b[1;33m                 raise Py4JJavaError(\n\u001b[0m\u001b[0;32m    327\u001b[0m                     \u001b[1;34m\"An error occurred while calling {0}{1}{2}.\\n\"\u001b[0m\u001b[1;33m.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    328\u001b[0m                     format(target_id, \".\", name), value)\n",
      "\u001b[1;31mPy4JJavaError\u001b[0m: An error occurred while calling o204.save.\n: java.lang.ClassNotFoundException: com.mysql.jdbc.Driver\r\n\tat java.net.URLClassLoader.findClass(Unknown Source)\r\n\tat java.lang.ClassLoader.loadClass(Unknown Source)\r\n\tat java.lang.ClassLoader.loadClass(Unknown Source)\r\n\tat org.apache.spark.sql.execution.datasources.jdbc.DriverRegistry$.register(DriverRegistry.scala:46)\r\n\tat org.apache.spark.sql.execution.datasources.jdbc.JDBCOptions.$anonfun$driverClass$1(JDBCOptions.scala:101)\r\n\tat org.apache.spark.sql.execution.datasources.jdbc.JDBCOptions.$anonfun$driverClass$1$adapted(JDBCOptions.scala:101)\r\n\tat scala.Option.foreach(Option.scala:407)\r\n\tat org.apache.spark.sql.execution.datasources.jdbc.JDBCOptions.<init>(JDBCOptions.scala:101)\r\n\tat org.apache.spark.sql.execution.datasources.jdbc.JdbcOptionsInWrite.<init>(JDBCOptions.scala:218)\r\n\tat org.apache.spark.sql.execution.datasources.jdbc.JdbcOptionsInWrite.<init>(JDBCOptions.scala:222)\r\n\tat org.apache.spark.sql.execution.datasources.jdbc.JdbcRelationProvider.createRelation(JdbcRelationProvider.scala:46)\r\n\tat org.apache.spark.sql.execution.datasources.SaveIntoDataSourceCommand.run(SaveIntoDataSourceCommand.scala:45)\r\n\tat org.apache.spark.sql.execution.command.ExecutedCommandExec.sideEffectResult$lzycompute(commands.scala:75)\r\n\tat org.apache.spark.sql.execution.command.ExecutedCommandExec.sideEffectResult(commands.scala:73)\r\n\tat org.apache.spark.sql.execution.command.ExecutedCommandExec.executeCollect(commands.scala:84)\r\n\tat org.apache.spark.sql.execution.QueryExecution$$anonfun$eagerlyExecuteCommands$1.$anonfun$applyOrElse$1(QueryExecution.scala:110)\r\n\tat org.apache.spark.sql.execution.SQLExecution$.$anonfun$withNewExecutionId$5(SQLExecution.scala:103)\r\n\tat org.apache.spark.sql.execution.SQLExecution$.withSQLConfPropagated(SQLExecution.scala:163)\r\n\tat org.apache.spark.sql.execution.SQLExecution$.$anonfun$withNewExecutionId$1(SQLExecution.scala:90)\r\n\tat org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:775)\r\n\tat org.apache.spark.sql.execution.SQLExecution$.withNewExecutionId(SQLExecution.scala:64)\r\n\tat org.apache.spark.sql.execution.QueryExecution$$anonfun$eagerlyExecuteCommands$1.applyOrElse(QueryExecution.scala:110)\r\n\tat org.apache.spark.sql.execution.QueryExecution$$anonfun$eagerlyExecuteCommands$1.applyOrElse(QueryExecution.scala:106)\r\n\tat org.apache.spark.sql.catalyst.trees.TreeNode.$anonfun$transformDownWithPruning$1(TreeNode.scala:481)\r\n\tat org.apache.spark.sql.catalyst.trees.CurrentOrigin$.withOrigin(TreeNode.scala:82)\r\n\tat org.apache.spark.sql.catalyst.trees.TreeNode.transformDownWithPruning(TreeNode.scala:481)\r\n\tat org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.org$apache$spark$sql$catalyst$plans$logical$AnalysisHelper$$super$transformDownWithPruning(LogicalPlan.scala:30)\r\n\tat org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.transformDownWithPruning(AnalysisHelper.scala:267)\r\n\tat org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.transformDownWithPruning$(AnalysisHelper.scala:263)\r\n\tat org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.transformDownWithPruning(LogicalPlan.scala:30)\r\n\tat org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.transformDownWithPruning(LogicalPlan.scala:30)\r\n\tat org.apache.spark.sql.catalyst.trees.TreeNode.transformDown(TreeNode.scala:457)\r\n\tat org.apache.spark.sql.execution.QueryExecution.eagerlyExecuteCommands(QueryExecution.scala:106)\r\n\tat org.apache.spark.sql.execution.QueryExecution.commandExecuted$lzycompute(QueryExecution.scala:93)\r\n\tat org.apache.spark.sql.execution.QueryExecution.commandExecuted(QueryExecution.scala:91)\r\n\tat org.apache.spark.sql.execution.QueryExecution.assertCommandExecuted(QueryExecution.scala:128)\r\n\tat org.apache.spark.sql.DataFrameWriter.runCommand(DataFrameWriter.scala:848)\r\n\tat org.apache.spark.sql.DataFrameWriter.saveToV1Source(DataFrameWriter.scala:382)\r\n\tat org.apache.spark.sql.DataFrameWriter.saveInternal(DataFrameWriter.scala:355)\r\n\tat org.apache.spark.sql.DataFrameWriter.save(DataFrameWriter.scala:247)\r\n\tat sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)\r\n\tat sun.reflect.NativeMethodAccessorImpl.invoke(Unknown Source)\r\n\tat sun.reflect.DelegatingMethodAccessorImpl.invoke(Unknown Source)\r\n\tat java.lang.reflect.Method.invoke(Unknown Source)\r\n\tat py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244)\r\n\tat py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:357)\r\n\tat py4j.Gateway.invoke(Gateway.java:282)\r\n\tat py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132)\r\n\tat py4j.commands.CallCommand.execute(CallCommand.java:79)\r\n\tat py4j.ClientServerConnection.waitForCommands(ClientServerConnection.java:182)\r\n\tat py4j.ClientServerConnection.run(ClientServerConnection.java:106)\r\n\tat java.lang.Thread.run(Unknown Source)\r\n"
     ]
    }
   ],
   "source": [
    "test(df_production)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "ename": "Py4JJavaError",
     "evalue": "An error occurred while calling o210.save.\n: org.apache.spark.SparkException: Job aborted.\r\n\tat org.apache.spark.sql.errors.QueryExecutionErrors$.jobAbortedError(QueryExecutionErrors.scala:496)\r\n\tat org.apache.spark.sql.execution.datasources.FileFormatWriter$.write(FileFormatWriter.scala:251)\r\n\tat org.apache.spark.sql.execution.datasources.InsertIntoHadoopFsRelationCommand.run(InsertIntoHadoopFsRelationCommand.scala:186)\r\n\tat org.apache.spark.sql.execution.command.DataWritingCommandExec.sideEffectResult$lzycompute(commands.scala:113)\r\n\tat org.apache.spark.sql.execution.command.DataWritingCommandExec.sideEffectResult(commands.scala:111)\r\n\tat org.apache.spark.sql.execution.command.DataWritingCommandExec.executeCollect(commands.scala:125)\r\n\tat org.apache.spark.sql.execution.QueryExecution$$anonfun$eagerlyExecuteCommands$1.$anonfun$applyOrElse$1(QueryExecution.scala:110)\r\n\tat org.apache.spark.sql.execution.SQLExecution$.$anonfun$withNewExecutionId$5(SQLExecution.scala:103)\r\n\tat org.apache.spark.sql.execution.SQLExecution$.withSQLConfPropagated(SQLExecution.scala:163)\r\n\tat org.apache.spark.sql.execution.SQLExecution$.$anonfun$withNewExecutionId$1(SQLExecution.scala:90)\r\n\tat org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:775)\r\n\tat org.apache.spark.sql.execution.SQLExecution$.withNewExecutionId(SQLExecution.scala:64)\r\n\tat org.apache.spark.sql.execution.QueryExecution$$anonfun$eagerlyExecuteCommands$1.applyOrElse(QueryExecution.scala:110)\r\n\tat org.apache.spark.sql.execution.QueryExecution$$anonfun$eagerlyExecuteCommands$1.applyOrElse(QueryExecution.scala:106)\r\n\tat org.apache.spark.sql.catalyst.trees.TreeNode.$anonfun$transformDownWithPruning$1(TreeNode.scala:481)\r\n\tat org.apache.spark.sql.catalyst.trees.CurrentOrigin$.withOrigin(TreeNode.scala:82)\r\n\tat org.apache.spark.sql.catalyst.trees.TreeNode.transformDownWithPruning(TreeNode.scala:481)\r\n\tat org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.org$apache$spark$sql$catalyst$plans$logical$AnalysisHelper$$super$transformDownWithPruning(LogicalPlan.scala:30)\r\n\tat org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.transformDownWithPruning(AnalysisHelper.scala:267)\r\n\tat org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.transformDownWithPruning$(AnalysisHelper.scala:263)\r\n\tat org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.transformDownWithPruning(LogicalPlan.scala:30)\r\n\tat org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.transformDownWithPruning(LogicalPlan.scala:30)\r\n\tat org.apache.spark.sql.catalyst.trees.TreeNode.transformDown(TreeNode.scala:457)\r\n\tat org.apache.spark.sql.execution.QueryExecution.eagerlyExecuteCommands(QueryExecution.scala:106)\r\n\tat org.apache.spark.sql.execution.QueryExecution.commandExecuted$lzycompute(QueryExecution.scala:93)\r\n\tat org.apache.spark.sql.execution.QueryExecution.commandExecuted(QueryExecution.scala:91)\r\n\tat org.apache.spark.sql.execution.QueryExecution.assertCommandExecuted(QueryExecution.scala:128)\r\n\tat org.apache.spark.sql.DataFrameWriter.runCommand(DataFrameWriter.scala:848)\r\n\tat org.apache.spark.sql.DataFrameWriter.saveToV1Source(DataFrameWriter.scala:382)\r\n\tat org.apache.spark.sql.DataFrameWriter.saveInternal(DataFrameWriter.scala:355)\r\n\tat org.apache.spark.sql.DataFrameWriter.save(DataFrameWriter.scala:239)\r\n\tat sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)\r\n\tat sun.reflect.NativeMethodAccessorImpl.invoke(Unknown Source)\r\n\tat sun.reflect.DelegatingMethodAccessorImpl.invoke(Unknown Source)\r\n\tat java.lang.reflect.Method.invoke(Unknown Source)\r\n\tat py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244)\r\n\tat py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:357)\r\n\tat py4j.Gateway.invoke(Gateway.java:282)\r\n\tat py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132)\r\n\tat py4j.commands.CallCommand.execute(CallCommand.java:79)\r\n\tat py4j.ClientServerConnection.waitForCommands(ClientServerConnection.java:182)\r\n\tat py4j.ClientServerConnection.run(ClientServerConnection.java:106)\r\n\tat java.lang.Thread.run(Unknown Source)\r\nCaused by: java.lang.UnsatisfiedLinkError: org.apache.hadoop.io.nativeio.NativeIO$Windows.access0(Ljava/lang/String;I)Z\r\n\tat org.apache.hadoop.io.nativeio.NativeIO$Windows.access0(Native Method)\r\n\tat org.apache.hadoop.io.nativeio.NativeIO$Windows.access(NativeIO.java:793)\r\n\tat org.apache.hadoop.fs.FileUtil.canRead(FileUtil.java:1215)\r\n\tat org.apache.hadoop.fs.FileUtil.list(FileUtil.java:1420)\r\n\tat org.apache.hadoop.fs.RawLocalFileSystem.listStatus(RawLocalFileSystem.java:601)\r\n\tat org.apache.hadoop.fs.FileSystem.listStatus(FileSystem.java:1972)\r\n\tat org.apache.hadoop.fs.FileSystem.listStatus(FileSystem.java:2014)\r\n\tat org.apache.hadoop.fs.ChecksumFileSystem.listStatus(ChecksumFileSystem.java:761)\r\n\tat org.apache.hadoop.fs.FileSystem.listStatus(FileSystem.java:1972)\r\n\tat org.apache.hadoop.fs.FileSystem.listStatus(FileSystem.java:2014)\r\n\tat org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter.getAllCommittedTaskPaths(FileOutputCommitter.java:334)\r\n\tat org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter.commitJobInternal(FileOutputCommitter.java:404)\r\n\tat org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter.commitJob(FileOutputCommitter.java:377)\r\n\tat org.apache.spark.internal.io.HadoopMapReduceCommitProtocol.commitJob(HadoopMapReduceCommitProtocol.scala:182)\r\n\tat org.apache.spark.sql.execution.datasources.FileFormatWriter$.$anonfun$write$20(FileFormatWriter.scala:240)\r\n\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\r\n\tat org.apache.spark.util.Utils$.timeTakenMs(Utils.scala:605)\r\n\tat org.apache.spark.sql.execution.datasources.FileFormatWriter$.write(FileFormatWriter.scala:240)\r\n\t... 41 more\r\n",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mPy4JJavaError\u001b[0m                             Traceback (most recent call last)",
      "\u001b[1;32mC:\\Users\\DEADEL~1\\AppData\\Local\\Temp/ipykernel_20300/3146936346.py\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[0msaveDataFrame\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdf_production\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfile\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;32mC:\\Users\\DEADEL~1\\AppData\\Local\\Temp/ipykernel_20300/3786061522.py\u001b[0m in \u001b[0;36msaveDataFrame\u001b[1;34m(dataframe, file_name)\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[1;32mdef\u001b[0m \u001b[0msaveDataFrame\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdataframe\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfile_name\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      2\u001b[0m     \u001b[0mfile\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;34mf\"data/refined-file/{file_name}.csv\"\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 3\u001b[1;33m     \u001b[0mdataframe\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mwrite\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mformat\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'csv'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0moption\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'header'\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;32mTrue\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmode\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'overwrite'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0moption\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'sep'\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;34m','\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msave\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mfile\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      4\u001b[0m     \u001b[1;32mreturn\u001b[0m \u001b[1;34mf\"{file_name} as been succesfully create\"\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\spark-3.2.1-bin-hadoop3.2\\python\\pyspark\\sql\\readwriter.py\u001b[0m in \u001b[0;36msave\u001b[1;34m(self, path, format, mode, partitionBy, **options)\u001b[0m\n\u001b[0;32m    738\u001b[0m             \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_jwrite\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msave\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    739\u001b[0m         \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 740\u001b[1;33m             \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_jwrite\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msave\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mpath\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    741\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    742\u001b[0m     \u001b[1;33m@\u001b[0m\u001b[0msince\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;36m1.4\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\spark-3.2.1-bin-hadoop3.2\\python\\lib\\py4j-0.10.9.3-src.zip\\py4j\\java_gateway.py\u001b[0m in \u001b[0;36m__call__\u001b[1;34m(self, *args)\u001b[0m\n\u001b[0;32m   1319\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1320\u001b[0m         \u001b[0manswer\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mgateway_client\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msend_command\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mcommand\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1321\u001b[1;33m         return_value = get_return_value(\n\u001b[0m\u001b[0;32m   1322\u001b[0m             answer, self.gateway_client, self.target_id, self.name)\n\u001b[0;32m   1323\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\spark-3.2.1-bin-hadoop3.2\\python\\pyspark\\sql\\utils.py\u001b[0m in \u001b[0;36mdeco\u001b[1;34m(*a, **kw)\u001b[0m\n\u001b[0;32m    109\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0mdeco\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0ma\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkw\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    110\u001b[0m         \u001b[1;32mtry\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 111\u001b[1;33m             \u001b[1;32mreturn\u001b[0m \u001b[0mf\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0ma\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkw\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    112\u001b[0m         \u001b[1;32mexcept\u001b[0m \u001b[0mpy4j\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mprotocol\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mPy4JJavaError\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    113\u001b[0m             \u001b[0mconverted\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mconvert_exception\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0me\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mjava_exception\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\spark-3.2.1-bin-hadoop3.2\\python\\lib\\py4j-0.10.9.3-src.zip\\py4j\\protocol.py\u001b[0m in \u001b[0;36mget_return_value\u001b[1;34m(answer, gateway_client, target_id, name)\u001b[0m\n\u001b[0;32m    324\u001b[0m             \u001b[0mvalue\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mOUTPUT_CONVERTER\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mtype\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0manswer\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m2\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mgateway_client\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    325\u001b[0m             \u001b[1;32mif\u001b[0m \u001b[0manswer\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m==\u001b[0m \u001b[0mREFERENCE_TYPE\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 326\u001b[1;33m                 raise Py4JJavaError(\n\u001b[0m\u001b[0;32m    327\u001b[0m                     \u001b[1;34m\"An error occurred while calling {0}{1}{2}.\\n\"\u001b[0m\u001b[1;33m.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    328\u001b[0m                     format(target_id, \".\", name), value)\n",
      "\u001b[1;31mPy4JJavaError\u001b[0m: An error occurred while calling o210.save.\n: org.apache.spark.SparkException: Job aborted.\r\n\tat org.apache.spark.sql.errors.QueryExecutionErrors$.jobAbortedError(QueryExecutionErrors.scala:496)\r\n\tat org.apache.spark.sql.execution.datasources.FileFormatWriter$.write(FileFormatWriter.scala:251)\r\n\tat org.apache.spark.sql.execution.datasources.InsertIntoHadoopFsRelationCommand.run(InsertIntoHadoopFsRelationCommand.scala:186)\r\n\tat org.apache.spark.sql.execution.command.DataWritingCommandExec.sideEffectResult$lzycompute(commands.scala:113)\r\n\tat org.apache.spark.sql.execution.command.DataWritingCommandExec.sideEffectResult(commands.scala:111)\r\n\tat org.apache.spark.sql.execution.command.DataWritingCommandExec.executeCollect(commands.scala:125)\r\n\tat org.apache.spark.sql.execution.QueryExecution$$anonfun$eagerlyExecuteCommands$1.$anonfun$applyOrElse$1(QueryExecution.scala:110)\r\n\tat org.apache.spark.sql.execution.SQLExecution$.$anonfun$withNewExecutionId$5(SQLExecution.scala:103)\r\n\tat org.apache.spark.sql.execution.SQLExecution$.withSQLConfPropagated(SQLExecution.scala:163)\r\n\tat org.apache.spark.sql.execution.SQLExecution$.$anonfun$withNewExecutionId$1(SQLExecution.scala:90)\r\n\tat org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:775)\r\n\tat org.apache.spark.sql.execution.SQLExecution$.withNewExecutionId(SQLExecution.scala:64)\r\n\tat org.apache.spark.sql.execution.QueryExecution$$anonfun$eagerlyExecuteCommands$1.applyOrElse(QueryExecution.scala:110)\r\n\tat org.apache.spark.sql.execution.QueryExecution$$anonfun$eagerlyExecuteCommands$1.applyOrElse(QueryExecution.scala:106)\r\n\tat org.apache.spark.sql.catalyst.trees.TreeNode.$anonfun$transformDownWithPruning$1(TreeNode.scala:481)\r\n\tat org.apache.spark.sql.catalyst.trees.CurrentOrigin$.withOrigin(TreeNode.scala:82)\r\n\tat org.apache.spark.sql.catalyst.trees.TreeNode.transformDownWithPruning(TreeNode.scala:481)\r\n\tat org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.org$apache$spark$sql$catalyst$plans$logical$AnalysisHelper$$super$transformDownWithPruning(LogicalPlan.scala:30)\r\n\tat org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.transformDownWithPruning(AnalysisHelper.scala:267)\r\n\tat org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.transformDownWithPruning$(AnalysisHelper.scala:263)\r\n\tat org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.transformDownWithPruning(LogicalPlan.scala:30)\r\n\tat org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.transformDownWithPruning(LogicalPlan.scala:30)\r\n\tat org.apache.spark.sql.catalyst.trees.TreeNode.transformDown(TreeNode.scala:457)\r\n\tat org.apache.spark.sql.execution.QueryExecution.eagerlyExecuteCommands(QueryExecution.scala:106)\r\n\tat org.apache.spark.sql.execution.QueryExecution.commandExecuted$lzycompute(QueryExecution.scala:93)\r\n\tat org.apache.spark.sql.execution.QueryExecution.commandExecuted(QueryExecution.scala:91)\r\n\tat org.apache.spark.sql.execution.QueryExecution.assertCommandExecuted(QueryExecution.scala:128)\r\n\tat org.apache.spark.sql.DataFrameWriter.runCommand(DataFrameWriter.scala:848)\r\n\tat org.apache.spark.sql.DataFrameWriter.saveToV1Source(DataFrameWriter.scala:382)\r\n\tat org.apache.spark.sql.DataFrameWriter.saveInternal(DataFrameWriter.scala:355)\r\n\tat org.apache.spark.sql.DataFrameWriter.save(DataFrameWriter.scala:239)\r\n\tat sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)\r\n\tat sun.reflect.NativeMethodAccessorImpl.invoke(Unknown Source)\r\n\tat sun.reflect.DelegatingMethodAccessorImpl.invoke(Unknown Source)\r\n\tat java.lang.reflect.Method.invoke(Unknown Source)\r\n\tat py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244)\r\n\tat py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:357)\r\n\tat py4j.Gateway.invoke(Gateway.java:282)\r\n\tat py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132)\r\n\tat py4j.commands.CallCommand.execute(CallCommand.java:79)\r\n\tat py4j.ClientServerConnection.waitForCommands(ClientServerConnection.java:182)\r\n\tat py4j.ClientServerConnection.run(ClientServerConnection.java:106)\r\n\tat java.lang.Thread.run(Unknown Source)\r\nCaused by: java.lang.UnsatisfiedLinkError: org.apache.hadoop.io.nativeio.NativeIO$Windows.access0(Ljava/lang/String;I)Z\r\n\tat org.apache.hadoop.io.nativeio.NativeIO$Windows.access0(Native Method)\r\n\tat org.apache.hadoop.io.nativeio.NativeIO$Windows.access(NativeIO.java:793)\r\n\tat org.apache.hadoop.fs.FileUtil.canRead(FileUtil.java:1215)\r\n\tat org.apache.hadoop.fs.FileUtil.list(FileUtil.java:1420)\r\n\tat org.apache.hadoop.fs.RawLocalFileSystem.listStatus(RawLocalFileSystem.java:601)\r\n\tat org.apache.hadoop.fs.FileSystem.listStatus(FileSystem.java:1972)\r\n\tat org.apache.hadoop.fs.FileSystem.listStatus(FileSystem.java:2014)\r\n\tat org.apache.hadoop.fs.ChecksumFileSystem.listStatus(ChecksumFileSystem.java:761)\r\n\tat org.apache.hadoop.fs.FileSystem.listStatus(FileSystem.java:1972)\r\n\tat org.apache.hadoop.fs.FileSystem.listStatus(FileSystem.java:2014)\r\n\tat org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter.getAllCommittedTaskPaths(FileOutputCommitter.java:334)\r\n\tat org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter.commitJobInternal(FileOutputCommitter.java:404)\r\n\tat org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter.commitJob(FileOutputCommitter.java:377)\r\n\tat org.apache.spark.internal.io.HadoopMapReduceCommitProtocol.commitJob(HadoopMapReduceCommitProtocol.scala:182)\r\n\tat org.apache.spark.sql.execution.datasources.FileFormatWriter$.$anonfun$write$20(FileFormatWriter.scala:240)\r\n\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\r\n\tat org.apache.spark.util.Utils$.timeTakenMs(Utils.scala:605)\r\n\tat org.apache.spark.sql.execution.datasources.FileFormatWriter$.write(FileFormatWriter.scala:240)\r\n\t... 41 more\r\n"
     ]
    }
   ],
   "source": [
    "saveDataFrame(df_production, file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "saveDataFrame(df_consomation, file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
