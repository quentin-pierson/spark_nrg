{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Installation des libairies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting findspark\n",
      "  Using cached findspark-2.0.0-py2.py3-none-any.whl (4.4 kB)\n",
      "Installing collected packages: findspark\n",
      "Successfully installed findspark-2.0.0\n",
      "Requirement already satisfied: pyspark in c:\\users\\antoine\\anaconda3\\lib\\site-packages (3.2.1)\n",
      "Requirement already satisfied: py4j==0.10.9.3 in c:\\users\\antoine\\anaconda3\\lib\\site-packages (from pyspark) (0.10.9.3)\n",
      "Requirement already satisfied: streamlit in c:\\users\\antoine\\anaconda3\\lib\\site-packages (1.5.0)\n",
      "Requirement already satisfied: tornado>=5.0 in c:\\users\\antoine\\anaconda3\\lib\\site-packages (from streamlit) (6.0.4)\n",
      "Requirement already satisfied: requests in c:\\users\\antoine\\anaconda3\\lib\\site-packages (from streamlit) (2.24.0)\n",
      "Requirement already satisfied: tzlocal in c:\\users\\antoine\\anaconda3\\lib\\site-packages (from streamlit) (4.1)\n",
      "Requirement already satisfied: altair>=3.2.0 in c:\\users\\antoine\\anaconda3\\lib\\site-packages (from streamlit) (4.2.0)\n",
      "Requirement already satisfied: blinker in c:\\users\\antoine\\anaconda3\\lib\\site-packages (from streamlit) (1.4)\n",
      "Requirement already satisfied: toml in c:\\users\\antoine\\anaconda3\\lib\\site-packages (from streamlit) (0.10.1)\n",
      "Requirement already satisfied: protobuf!=3.11,>=3.6.0 in c:\\users\\antoine\\anaconda3\\lib\\site-packages (from streamlit) (3.19.4)\n",
      "Requirement already satisfied: pympler>=0.9 in c:\\users\\antoine\\anaconda3\\lib\\site-packages (from streamlit) (1.0.1)\n",
      "Requirement already satisfied: click>=7.0 in c:\\users\\antoine\\anaconda3\\lib\\site-packages (from streamlit) (7.1.2)\n",
      "Requirement already satisfied: watchdog; platform_system != \"Darwin\" in c:\\users\\antoine\\anaconda3\\lib\\site-packages (from streamlit) (0.10.3)\n",
      "Requirement already satisfied: pydeck>=0.1.dev5 in c:\\users\\antoine\\anaconda3\\lib\\site-packages (from streamlit) (0.7.1)\n",
      "Requirement already satisfied: attrs in c:\\users\\antoine\\anaconda3\\lib\\site-packages (from streamlit) (19.3.0)\n",
      "Requirement already satisfied: numpy in c:\\users\\antoine\\anaconda3\\lib\\site-packages (from streamlit) (1.18.5)\n",
      "Requirement already satisfied: cachetools>=4.0 in c:\\users\\antoine\\anaconda3\\lib\\site-packages (from streamlit) (5.0.0)\n",
      "Requirement already satisfied: packaging in c:\\users\\antoine\\anaconda3\\lib\\site-packages (from streamlit) (20.4)\n",
      "Requirement already satisfied: base58 in c:\\users\\antoine\\anaconda3\\lib\\site-packages (from streamlit) (2.1.1)\n",
      "Requirement already satisfied: pandas>=0.21.0 in c:\\users\\antoine\\anaconda3\\lib\\site-packages (from streamlit) (1.0.5)\n",
      "Requirement already satisfied: astor in c:\\users\\antoine\\anaconda3\\lib\\site-packages (from streamlit) (0.8.1)\n",
      "Requirement already satisfied: pillow>=6.2.0 in c:\\users\\antoine\\anaconda3\\lib\\site-packages (from streamlit) (7.2.0)\n",
      "Requirement already satisfied: python-dateutil in c:\\users\\antoine\\anaconda3\\lib\\site-packages (from streamlit) (2.8.1)\n",
      "Requirement already satisfied: validators in c:\\users\\antoine\\anaconda3\\lib\\site-packages (from streamlit) (0.18.2)\n",
      "Requirement already satisfied: gitpython!=3.1.19 in c:\\users\\antoine\\anaconda3\\lib\\site-packages (from streamlit) (3.1.26)\n",
      "Requirement already satisfied: pyarrow in c:\\users\\antoine\\anaconda3\\lib\\site-packages (from streamlit) (7.0.0)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in c:\\users\\antoine\\anaconda3\\lib\\site-packages (from requests->streamlit) (2020.6.20)\n",
      "Requirement already satisfied: idna<3,>=2.5 in c:\\users\\antoine\\anaconda3\\lib\\site-packages (from requests->streamlit) (2.10)\n",
      "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in c:\\users\\antoine\\anaconda3\\lib\\site-packages (from requests->streamlit) (1.25.9)\n",
      "Requirement already satisfied: chardet<4,>=3.0.2 in c:\\users\\antoine\\anaconda3\\lib\\site-packages (from requests->streamlit) (3.0.4)\n",
      "Requirement already satisfied: pytz-deprecation-shim in c:\\users\\antoine\\anaconda3\\lib\\site-packages (from tzlocal->streamlit) (0.1.0.post0)\n",
      "Requirement already satisfied: backports.zoneinfo; python_version < \"3.9\" in c:\\users\\antoine\\anaconda3\\lib\\site-packages (from tzlocal->streamlit) (0.2.1)\n",
      "Requirement already satisfied: tzdata; platform_system == \"Windows\" in c:\\users\\antoine\\anaconda3\\lib\\site-packages (from tzlocal->streamlit) (2021.5)\n",
      "Requirement already satisfied: entrypoints in c:\\users\\antoine\\anaconda3\\lib\\site-packages (from altair>=3.2.0->streamlit) (0.3)\n",
      "Requirement already satisfied: jsonschema>=3.0 in c:\\users\\antoine\\anaconda3\\lib\\site-packages (from altair>=3.2.0->streamlit) (3.2.0)\n",
      "Requirement already satisfied: jinja2 in c:\\users\\antoine\\anaconda3\\lib\\site-packages (from altair>=3.2.0->streamlit) (2.11.2)\n",
      "Requirement already satisfied: toolz in c:\\users\\antoine\\anaconda3\\lib\\site-packages (from altair>=3.2.0->streamlit) (0.10.0)\n",
      "Requirement already satisfied: pathtools>=0.1.1 in c:\\users\\antoine\\anaconda3\\lib\\site-packages (from watchdog; platform_system != \"Darwin\"->streamlit) (0.1.2)\n",
      "Requirement already satisfied: ipywidgets>=7.0.0 in c:\\users\\antoine\\anaconda3\\lib\\site-packages (from pydeck>=0.1.dev5->streamlit) (7.5.1)\n",
      "Requirement already satisfied: traitlets>=4.3.2 in c:\\users\\antoine\\anaconda3\\lib\\site-packages (from pydeck>=0.1.dev5->streamlit) (4.3.3)\n",
      "Requirement already satisfied: ipykernel>=5.1.2; python_version >= \"3.4\" in c:\\users\\antoine\\anaconda3\\lib\\site-packages (from pydeck>=0.1.dev5->streamlit) (5.3.2)\n",
      "Requirement already satisfied: pyparsing>=2.0.2 in c:\\users\\antoine\\anaconda3\\lib\\site-packages (from packaging->streamlit) (2.4.7)\n",
      "Requirement already satisfied: six in c:\\users\\antoine\\anaconda3\\lib\\site-packages (from packaging->streamlit) (1.15.0)\n",
      "Requirement already satisfied: pytz>=2017.2 in c:\\users\\antoine\\anaconda3\\lib\\site-packages (from pandas>=0.21.0->streamlit) (2020.1)\n",
      "Requirement already satisfied: decorator>=3.4.0 in c:\\users\\antoine\\anaconda3\\lib\\site-packages (from validators->streamlit) (4.4.2)\n",
      "Requirement already satisfied: gitdb<5,>=4.0.1 in c:\\users\\antoine\\anaconda3\\lib\\site-packages (from gitpython!=3.1.19->streamlit) (4.0.9)\n",
      "Requirement already satisfied: setuptools in c:\\users\\antoine\\anaconda3\\lib\\site-packages (from jsonschema>=3.0->altair>=3.2.0->streamlit) (49.2.0.post20200714)\n",
      "Requirement already satisfied: pyrsistent>=0.14.0 in c:\\users\\antoine\\anaconda3\\lib\\site-packages (from jsonschema>=3.0->altair>=3.2.0->streamlit) (0.16.0)\n",
      "Requirement already satisfied: MarkupSafe>=0.23 in c:\\users\\antoine\\anaconda3\\lib\\site-packages (from jinja2->altair>=3.2.0->streamlit) (1.1.1)\n",
      "Requirement already satisfied: widgetsnbextension~=3.5.0 in c:\\users\\antoine\\anaconda3\\lib\\site-packages (from ipywidgets>=7.0.0->pydeck>=0.1.dev5->streamlit) (3.5.1)\n",
      "Requirement already satisfied: ipython>=4.0.0; python_version >= \"3.3\" in c:\\users\\antoine\\anaconda3\\lib\\site-packages (from ipywidgets>=7.0.0->pydeck>=0.1.dev5->streamlit) (7.31.1)\n",
      "Requirement already satisfied: nbformat>=4.2.0 in c:\\users\\antoine\\anaconda3\\lib\\site-packages (from ipywidgets>=7.0.0->pydeck>=0.1.dev5->streamlit) (5.0.7)\n",
      "Requirement already satisfied: ipython-genutils in c:\\users\\antoine\\anaconda3\\lib\\site-packages (from traitlets>=4.3.2->pydeck>=0.1.dev5->streamlit) (0.2.0)\n",
      "Requirement already satisfied: jupyter-client in c:\\users\\antoine\\anaconda3\\lib\\site-packages (from ipykernel>=5.1.2; python_version >= \"3.4\"->pydeck>=0.1.dev5->streamlit) (6.1.6)\n",
      "Requirement already satisfied: smmap<6,>=3.0.1 in c:\\users\\antoine\\anaconda3\\lib\\site-packages (from gitdb<5,>=4.0.1->gitpython!=3.1.19->streamlit) (5.0.0)\n",
      "Requirement already satisfied: notebook>=4.4.1 in c:\\users\\antoine\\anaconda3\\lib\\site-packages (from widgetsnbextension~=3.5.0->ipywidgets>=7.0.0->pydeck>=0.1.dev5->streamlit) (6.0.3)\n",
      "Requirement already satisfied: pygments in c:\\users\\antoine\\anaconda3\\lib\\site-packages (from ipython>=4.0.0; python_version >= \"3.3\"->ipywidgets>=7.0.0->pydeck>=0.1.dev5->streamlit) (2.6.1)\n",
      "Requirement already satisfied: pickleshare in c:\\users\\antoine\\anaconda3\\lib\\site-packages (from ipython>=4.0.0; python_version >= \"3.3\"->ipywidgets>=7.0.0->pydeck>=0.1.dev5->streamlit) (0.7.5)\n",
      "Requirement already satisfied: colorama; sys_platform == \"win32\" in c:\\users\\antoine\\anaconda3\\lib\\site-packages (from ipython>=4.0.0; python_version >= \"3.3\"->ipywidgets>=7.0.0->pydeck>=0.1.dev5->streamlit) (0.4.3)\n",
      "Requirement already satisfied: backcall in c:\\users\\antoine\\anaconda3\\lib\\site-packages (from ipython>=4.0.0; python_version >= \"3.3\"->ipywidgets>=7.0.0->pydeck>=0.1.dev5->streamlit) (0.2.0)\n",
      "Requirement already satisfied: jedi>=0.16 in c:\\users\\antoine\\anaconda3\\lib\\site-packages (from ipython>=4.0.0; python_version >= \"3.3\"->ipywidgets>=7.0.0->pydeck>=0.1.dev5->streamlit) (0.17.1)\n",
      "Requirement already satisfied: matplotlib-inline in c:\\users\\antoine\\anaconda3\\lib\\site-packages (from ipython>=4.0.0; python_version >= \"3.3\"->ipywidgets>=7.0.0->pydeck>=0.1.dev5->streamlit) (0.1.3)\n",
      "Requirement already satisfied: prompt-toolkit!=3.0.0,!=3.0.1,<3.1.0,>=2.0.0 in c:\\users\\antoine\\anaconda3\\lib\\site-packages (from ipython>=4.0.0; python_version >= \"3.3\"->ipywidgets>=7.0.0->pydeck>=0.1.dev5->streamlit) (3.0.5)\n",
      "Requirement already satisfied: jupyter-core in c:\\users\\antoine\\anaconda3\\lib\\site-packages (from nbformat>=4.2.0->ipywidgets>=7.0.0->pydeck>=0.1.dev5->streamlit) (4.6.3)\n",
      "Requirement already satisfied: pyzmq>=13 in c:\\users\\antoine\\anaconda3\\lib\\site-packages (from jupyter-client->ipykernel>=5.1.2; python_version >= \"3.4\"->pydeck>=0.1.dev5->streamlit) (19.0.1)\n",
      "Requirement already satisfied: terminado>=0.8.1 in c:\\users\\antoine\\anaconda3\\lib\\site-packages (from notebook>=4.4.1->widgetsnbextension~=3.5.0->ipywidgets>=7.0.0->pydeck>=0.1.dev5->streamlit) (0.8.3)\n",
      "Requirement already satisfied: prometheus-client in c:\\users\\antoine\\anaconda3\\lib\\site-packages (from notebook>=4.4.1->widgetsnbextension~=3.5.0->ipywidgets>=7.0.0->pydeck>=0.1.dev5->streamlit) (0.8.0)\n",
      "Requirement already satisfied: Send2Trash in c:\\users\\antoine\\anaconda3\\lib\\site-packages (from notebook>=4.4.1->widgetsnbextension~=3.5.0->ipywidgets>=7.0.0->pydeck>=0.1.dev5->streamlit) (1.5.0)\n",
      "Requirement already satisfied: nbconvert in c:\\users\\antoine\\anaconda3\\lib\\site-packages (from notebook>=4.4.1->widgetsnbextension~=3.5.0->ipywidgets>=7.0.0->pydeck>=0.1.dev5->streamlit) (5.6.1)\n",
      "Requirement already satisfied: parso<0.8.0,>=0.7.0 in c:\\users\\antoine\\anaconda3\\lib\\site-packages (from jedi>=0.16->ipython>=4.0.0; python_version >= \"3.3\"->ipywidgets>=7.0.0->pydeck>=0.1.dev5->streamlit) (0.7.0)\n",
      "Requirement already satisfied: wcwidth in c:\\users\\antoine\\anaconda3\\lib\\site-packages (from prompt-toolkit!=3.0.0,!=3.0.1,<3.1.0,>=2.0.0->ipython>=4.0.0; python_version >= \"3.3\"->ipywidgets>=7.0.0->pydeck>=0.1.dev5->streamlit) (0.2.5)\n",
      "Requirement already satisfied: pywin32>=1.0; sys_platform == \"win32\" in c:\\users\\antoine\\anaconda3\\lib\\site-packages (from jupyter-core->nbformat>=4.2.0->ipywidgets>=7.0.0->pydeck>=0.1.dev5->streamlit) (227)\n",
      "Requirement already satisfied: bleach in c:\\users\\antoine\\anaconda3\\lib\\site-packages (from nbconvert->notebook>=4.4.1->widgetsnbextension~=3.5.0->ipywidgets>=7.0.0->pydeck>=0.1.dev5->streamlit) (3.1.5)\n",
      "Requirement already satisfied: pandocfilters>=1.4.1 in c:\\users\\antoine\\anaconda3\\lib\\site-packages (from nbconvert->notebook>=4.4.1->widgetsnbextension~=3.5.0->ipywidgets>=7.0.0->pydeck>=0.1.dev5->streamlit) (1.4.2)\n",
      "Requirement already satisfied: testpath in c:\\users\\antoine\\anaconda3\\lib\\site-packages (from nbconvert->notebook>=4.4.1->widgetsnbextension~=3.5.0->ipywidgets>=7.0.0->pydeck>=0.1.dev5->streamlit) (0.4.4)\n",
      "Requirement already satisfied: mistune<2,>=0.8.1 in c:\\users\\antoine\\anaconda3\\lib\\site-packages (from nbconvert->notebook>=4.4.1->widgetsnbextension~=3.5.0->ipywidgets>=7.0.0->pydeck>=0.1.dev5->streamlit) (0.8.4)\n",
      "Requirement already satisfied: defusedxml in c:\\users\\antoine\\anaconda3\\lib\\site-packages (from nbconvert->notebook>=4.4.1->widgetsnbextension~=3.5.0->ipywidgets>=7.0.0->pydeck>=0.1.dev5->streamlit) (0.6.0)\n",
      "Requirement already satisfied: webencodings in c:\\users\\antoine\\anaconda3\\lib\\site-packages (from bleach->nbconvert->notebook>=4.4.1->widgetsnbextension~=3.5.0->ipywidgets>=7.0.0->pydeck>=0.1.dev5->streamlit) (0.5.1)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "ERROR: Could not find a version that satisfies the requirement nympy (from versions: none)\n",
      "ERROR: No matching distribution found for nympy\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting plotly\n",
      "  Using cached plotly-5.5.0-py2.py3-none-any.whl (26.5 MB)\n",
      "Requirement already satisfied: six in c:\\users\\antoine\\anaconda3\\lib\\site-packages (from plotly) (1.15.0)\n",
      "Collecting tenacity>=6.2.0\n",
      "  Downloading tenacity-8.0.1-py3-none-any.whl (24 kB)\n",
      "Installing collected packages: tenacity, plotly\n",
      "Successfully installed plotly-5.5.0 tenacity-8.0.1\n"
     ]
    }
   ],
   "source": [
    "#!pip install findspark\n",
    "#!pip install pyspark\n",
    "#!pip install streamlit\n",
    "#!pip install nympy\n",
    "#!pip install plotly"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Import des librairies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
<<<<<<< HEAD
       "'C:\\\\Users\\\\Antoine\\\\anaconda3\\\\lib\\\\site-packages\\\\pyspark'"
=======
       "'/Users/pierrick/opt/miniconda3/lib/python3.9/site-packages/pyspark'"
>>>>>>> c4cb42a8c993b074ac6a1cdfb0c076011684ce2b
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import findspark\n",
    "findspark.init()\n",
    "findspark.find()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "from pyspark import SparkContext\n",
    "import pyspark.sql.functions as spf\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "#import streamlit as st\n",
    "import plotly as plt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Traitement ETL\n",
    "## Création d'un SparkSession"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using Spark's default log4j profile: org/apache/spark/log4j-defaults.properties\n",
      "Setting default log level to \"WARN\".\n",
      "To adjust logging level use sc.setLogLevel(newLevel). For SparkR, use setLogLevel(newLevel).\n",
      "22/02/06 16:54:42 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n"
     ]
    },
    {
     "ename": "Py4JJavaError",
     "evalue": "An error occurred while calling None.org.apache.spark.api.java.JavaSparkContext.\n: java.lang.IllegalAccessError: class org.apache.spark.storage.StorageUtils$ (in unnamed module @0x3c60b7e7) cannot access class sun.nio.ch.DirectBuffer (in module java.base) because module java.base does not export sun.nio.ch to unnamed module @0x3c60b7e7\n\tat org.apache.spark.storage.StorageUtils$.<init>(StorageUtils.scala:213)\n\tat org.apache.spark.storage.StorageUtils$.<clinit>(StorageUtils.scala)\n\tat org.apache.spark.storage.BlockManagerMasterEndpoint.<init>(BlockManagerMasterEndpoint.scala:110)\n\tat org.apache.spark.SparkEnv$.$anonfun$create$9(SparkEnv.scala:348)\n\tat org.apache.spark.SparkEnv$.registerOrLookupEndpoint$1(SparkEnv.scala:287)\n\tat org.apache.spark.SparkEnv$.create(SparkEnv.scala:336)\n\tat org.apache.spark.SparkEnv$.createDriverEnv(SparkEnv.scala:191)\n\tat org.apache.spark.SparkContext.createSparkEnv(SparkContext.scala:277)\n\tat org.apache.spark.SparkContext.<init>(SparkContext.scala:460)\n\tat org.apache.spark.api.java.JavaSparkContext.<init>(JavaSparkContext.scala:58)\n\tat java.base/jdk.internal.reflect.NativeConstructorAccessorImpl.newInstance0(Native Method)\n\tat java.base/jdk.internal.reflect.NativeConstructorAccessorImpl.newInstance(NativeConstructorAccessorImpl.java:77)\n\tat java.base/jdk.internal.reflect.DelegatingConstructorAccessorImpl.newInstance(DelegatingConstructorAccessorImpl.java:45)\n\tat java.base/java.lang.reflect.Constructor.newInstanceWithCaller(Constructor.java:499)\n\tat java.base/java.lang.reflect.Constructor.newInstance(Constructor.java:480)\n\tat py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:247)\n\tat py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:357)\n\tat py4j.Gateway.invoke(Gateway.java:238)\n\tat py4j.commands.ConstructorCommand.invokeConstructor(ConstructorCommand.java:80)\n\tat py4j.commands.ConstructorCommand.execute(ConstructorCommand.java:69)\n\tat py4j.ClientServerConnection.waitForCommands(ClientServerConnection.java:182)\n\tat py4j.ClientServerConnection.run(ClientServerConnection.java:106)\n\tat java.base/java.lang.Thread.run(Thread.java:833)\n",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mPy4JJavaError\u001b[0m                             Traceback (most recent call last)",
      "\u001b[0;32m/var/folders/db/w9f9cb6n65dbtvnptzv2s2gw0000gn/T/ipykernel_7401/711131801.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mspark\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mSparkSession\u001b[0m\u001b[0;31m \u001b[0m\u001b[0;31m\\\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m     \u001b[0;34m.\u001b[0m\u001b[0mbuilder\u001b[0m\u001b[0;31m \u001b[0m\u001b[0;31m\\\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m     \u001b[0;34m.\u001b[0m\u001b[0mappName\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"how to read csv file\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;31m \u001b[0m\u001b[0;31m\\\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m     \u001b[0;34m.\u001b[0m\u001b[0mgetOrCreate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/opt/miniconda3/lib/python3.9/site-packages/pyspark/sql/session.py\u001b[0m in \u001b[0;36mgetOrCreate\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    226\u001b[0m                             \u001b[0msparkConf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mset\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mkey\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvalue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    227\u001b[0m                         \u001b[0;31m# This SparkContext may be an existing one.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 228\u001b[0;31m                         \u001b[0msc\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mSparkContext\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgetOrCreate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msparkConf\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    229\u001b[0m                     \u001b[0;31m# Do not update `SparkConf` for existing `SparkContext`, as it's shared\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    230\u001b[0m                     \u001b[0;31m# by all sessions.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/opt/miniconda3/lib/python3.9/site-packages/pyspark/context.py\u001b[0m in \u001b[0;36mgetOrCreate\u001b[0;34m(cls, conf)\u001b[0m\n\u001b[1;32m    390\u001b[0m         \u001b[0;32mwith\u001b[0m \u001b[0mSparkContext\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_lock\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    391\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mSparkContext\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_active_spark_context\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 392\u001b[0;31m                 \u001b[0mSparkContext\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mconf\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mconf\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0mSparkConf\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    393\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mSparkContext\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_active_spark_context\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    394\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/opt/miniconda3/lib/python3.9/site-packages/pyspark/context.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, master, appName, sparkHome, pyFiles, environment, batchSize, serializer, conf, gateway, jsc, profiler_cls)\u001b[0m\n\u001b[1;32m    144\u001b[0m         \u001b[0mSparkContext\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_ensure_initialized\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgateway\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mgateway\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mconf\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mconf\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    145\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 146\u001b[0;31m             self._do_init(master, appName, sparkHome, pyFiles, environment, batchSize, serializer,\n\u001b[0m\u001b[1;32m    147\u001b[0m                           conf, jsc, profiler_cls)\n\u001b[1;32m    148\u001b[0m         \u001b[0;32mexcept\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/opt/miniconda3/lib/python3.9/site-packages/pyspark/context.py\u001b[0m in \u001b[0;36m_do_init\u001b[0;34m(self, master, appName, sparkHome, pyFiles, environment, batchSize, serializer, conf, jsc, profiler_cls)\u001b[0m\n\u001b[1;32m    207\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    208\u001b[0m         \u001b[0;31m# Create the Java SparkContext through Py4J\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 209\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_jsc\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mjsc\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_initialize_context\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_conf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_jconf\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    210\u001b[0m         \u001b[0;31m# Reset the SparkConf to the one actually used by the SparkContext in JVM.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    211\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_conf\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mSparkConf\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0m_jconf\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_jsc\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mconf\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/opt/miniconda3/lib/python3.9/site-packages/pyspark/context.py\u001b[0m in \u001b[0;36m_initialize_context\u001b[0;34m(self, jconf)\u001b[0m\n\u001b[1;32m    327\u001b[0m         \u001b[0mInitialize\u001b[0m \u001b[0mSparkContext\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mfunction\u001b[0m \u001b[0mto\u001b[0m \u001b[0mallow\u001b[0m \u001b[0msubclass\u001b[0m \u001b[0mspecific\u001b[0m \u001b[0minitialization\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    328\u001b[0m         \"\"\"\n\u001b[0;32m--> 329\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_jvm\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mJavaSparkContext\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mjconf\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    330\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    331\u001b[0m     \u001b[0;34m@\u001b[0m\u001b[0mclassmethod\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/opt/miniconda3/lib/python3.9/site-packages/py4j/java_gateway.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *args)\u001b[0m\n\u001b[1;32m   1583\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1584\u001b[0m         \u001b[0manswer\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_gateway_client\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msend_command\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcommand\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1585\u001b[0;31m         return_value = get_return_value(\n\u001b[0m\u001b[1;32m   1586\u001b[0m             answer, self._gateway_client, None, self._fqn)\n\u001b[1;32m   1587\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/opt/miniconda3/lib/python3.9/site-packages/py4j/protocol.py\u001b[0m in \u001b[0;36mget_return_value\u001b[0;34m(answer, gateway_client, target_id, name)\u001b[0m\n\u001b[1;32m    324\u001b[0m             \u001b[0mvalue\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mOUTPUT_CONVERTER\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mtype\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0manswer\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m2\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgateway_client\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    325\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0manswer\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0mREFERENCE_TYPE\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 326\u001b[0;31m                 raise Py4JJavaError(\n\u001b[0m\u001b[1;32m    327\u001b[0m                     \u001b[0;34m\"An error occurred while calling {0}{1}{2}.\\n\"\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    328\u001b[0m                     format(target_id, \".\", name), value)\n",
      "\u001b[0;31mPy4JJavaError\u001b[0m: An error occurred while calling None.org.apache.spark.api.java.JavaSparkContext.\n: java.lang.IllegalAccessError: class org.apache.spark.storage.StorageUtils$ (in unnamed module @0x3c60b7e7) cannot access class sun.nio.ch.DirectBuffer (in module java.base) because module java.base does not export sun.nio.ch to unnamed module @0x3c60b7e7\n\tat org.apache.spark.storage.StorageUtils$.<init>(StorageUtils.scala:213)\n\tat org.apache.spark.storage.StorageUtils$.<clinit>(StorageUtils.scala)\n\tat org.apache.spark.storage.BlockManagerMasterEndpoint.<init>(BlockManagerMasterEndpoint.scala:110)\n\tat org.apache.spark.SparkEnv$.$anonfun$create$9(SparkEnv.scala:348)\n\tat org.apache.spark.SparkEnv$.registerOrLookupEndpoint$1(SparkEnv.scala:287)\n\tat org.apache.spark.SparkEnv$.create(SparkEnv.scala:336)\n\tat org.apache.spark.SparkEnv$.createDriverEnv(SparkEnv.scala:191)\n\tat org.apache.spark.SparkContext.createSparkEnv(SparkContext.scala:277)\n\tat org.apache.spark.SparkContext.<init>(SparkContext.scala:460)\n\tat org.apache.spark.api.java.JavaSparkContext.<init>(JavaSparkContext.scala:58)\n\tat java.base/jdk.internal.reflect.NativeConstructorAccessorImpl.newInstance0(Native Method)\n\tat java.base/jdk.internal.reflect.NativeConstructorAccessorImpl.newInstance(NativeConstructorAccessorImpl.java:77)\n\tat java.base/jdk.internal.reflect.DelegatingConstructorAccessorImpl.newInstance(DelegatingConstructorAccessorImpl.java:45)\n\tat java.base/java.lang.reflect.Constructor.newInstanceWithCaller(Constructor.java:499)\n\tat java.base/java.lang.reflect.Constructor.newInstance(Constructor.java:480)\n\tat py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:247)\n\tat py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:357)\n\tat py4j.Gateway.invoke(Gateway.java:238)\n\tat py4j.commands.ConstructorCommand.invokeConstructor(ConstructorCommand.java:80)\n\tat py4j.commands.ConstructorCommand.execute(ConstructorCommand.java:69)\n\tat py4j.ClientServerConnection.waitForCommands(ClientServerConnection.java:182)\n\tat py4j.ClientServerConnection.run(ClientServerConnection.java:106)\n\tat java.base/java.lang.Thread.run(Thread.java:833)\n"
     ]
    }
   ],
   "source": [
    "spark = SparkSession \\\n",
    "    .builder \\\n",
    "    .appName(\"how to read csv file\") \\\n",
    "    .getOrCreate()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Fonction d'ouverture de fichier"
   ]
  },
  {
   "cell_type": "code",
<<<<<<< HEAD
   "execution_count": 6,
=======
   "execution_count": null,
>>>>>>> c4cb42a8c993b074ac6a1cdfb0c076011684ce2b
   "metadata": {},
   "outputs": [],
   "source": [
    "def openDataFrame(data):\n",
    "    df = spark.read.option(\"delimiter\", \",\") \\\n",
    "        .csv(f\"data/raw-file/{data}.csv\", header=True)\n",
    "    df.printSchema()\n",
    "    return df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Liste des fichiers à traiter"
   ]
  },
  {
   "cell_type": "code",
<<<<<<< HEAD
   "execution_count": 7,
=======
   "execution_count": null,
>>>>>>> c4cb42a8c993b074ac6a1cdfb0c076011684ce2b
   "metadata": {},
   "outputs": [],
   "source": [
    "fileName = [\"dre_auvergne_rhone_alpes\",\n",
    "            \"dre_bourgogne_franche_comte\",\n",
    "            \"dre_bretagne\",\n",
    "            \"dre_centre_val_de_loire\",\n",
    "            \"dre_corse\",\n",
    "            \"dre_france_metropolitaine\",\n",
    "            \"dre_grand_est\",\n",
    "            \"dre_hauts_de_france\",\n",
    "            \"dre_ile_de_france\",\n",
    "            \"dre_normandie\",\n",
    "            \"dre_nouvelle_aquitaine\",\n",
    "            \"dre_occitanie\",\n",
    "            \"dre_pays_de_la_loire\",\n",
    "            \"dre_provence_alpes_cote_d_azur\",\n",
    "           ]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Enlève la limitation de l'affichage du dataframe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "spark.conf.set('spark.sql.repl.eagerEval.enabled', True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Ouverture d'un fichier .csv et affichage de dataframe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = openDataFrame(fileName[0])\n",
    "df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Transposition de l'affichage"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "df.show(3, vertical=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Affichage des colonnes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.columns"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Affichage des données"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.printSchema()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "La deuxième colonne ne porte pas de nom ..\n",
    "Nous allons y remèdier maintenant 😉"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# def withColumnRenamed(existingName: String, newName: String): DataFrame\n",
    "df = df.withColumnRenamed(\"_c1\",\"Intitulé énergie\")\n",
    "df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Sommaire des données"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.select('2014', '2015').describe().show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Permière version des liste\n",
    "\n",
    "list_production = [\n",
    "    'P1','P2','P3','P4','P5','P7','P8','P9','P10','P11','P12','P13','P14',\n",
    "    'E1','E2','E4','E6','E7','E8','E9','E10','E11','E12','E13','E14','E15','E16','E17','E18','E19','E20','E21','E22','E23','E24','E25','E26','E27','E28'\n",
    "]\n",
    "\n",
    "list_consomation = [\n",
    "    'C1','CI1','CI3','CI3b','CI4','CI9','CI11','CI5','CI6','CI7','CI8',\n",
    "    'CTR1','CTR2','CTR3','CTR4','CTR5','CTR6','CTR7','CTR8','CTR9','CTR10','CTR11',\n",
    "    'CR1','CR7','CR2','CR3','CR4','CR5','CR6','CR8',\n",
    "    'CTE1','CTE7','CTE2','CTE3','CTE4','CTE5','CTE6','CTE8',\n",
    "    'CA1','CA7','CA2','CA21','CA3','CA4','CA5','CA6','CA8',\n",
    "    'C2','C3','C4','C13','C14'\n",
    "]\n",
    "\n",
    "# Penser à passer par une tranposition puis pour un passae d'indexe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_production = df.filter(df.PRODUCTION.isin(list_production))\n",
    "df_production.show()\n",
    "\n",
    "## Liste de test non concluant\n",
    "#subset_df = df.filter(df.PRODUCTION.isin(list_production))\n",
    "#df_production = spark.createDataFrame(subset_df, df.columns)\n",
    "#df_production = df.withColumn(df.columns[0], spf.when(df.PRODUCTION.isin(list_production)))\n",
    "#df_production.show"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_consomation = df.filter(df.PRODUCTION.isin(list_consomation))\n",
    "df_consomation.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Sauvegardes \n",
    "\n",
    "###  /!\\ En Chantier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def saveDataFrame(dataframe, file_name):\n",
    "    file = f\"data/refined-file/{file_name}.csv\"\n",
    "    dataframe.write.format('csv').option('header',True).mode('overwrite').option('sep',',').save(file)\n",
    "    return f\"{file_name} as been succesfully create\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def i(dataframe):\n",
    "    dataframe.write.format('jdbc').options(\n",
    "      url='jdbc:mysql://localhost/spark_nrg',\n",
    "      driver='com.mysql.jdbc.Driver',\n",
    "      dbtable='D_TEMP',\n",
    "      user='user',\n",
    "      password='user').mode('append').save()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "file = f\"production_{fileName[0]}\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test(df_production)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "saveDataFrame(df_production, file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "saveDataFrame(df_consomation, file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
<<<<<<< HEAD
   "version": "3.8.3"
=======
   "version": "3.9.1"
>>>>>>> c4cb42a8c993b074ac6a1cdfb0c076011684ce2b
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
