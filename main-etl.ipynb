{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Installation des libairies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "#!pip install findspark\n",
    "#!pip install pyspark\n",
    "#!pip install streamlit\n",
    "#!pip install nympy\n",
    "#!pip install plotly"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Import des librairies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'c:\\\\users\\\\antoine\\\\appdata\\\\local\\\\programs\\\\python\\\\python38-32\\\\lib\\\\site-packages\\\\pyspark'"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import findspark\n",
    "findspark.init()\n",
    "findspark.find()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "from pyspark import SparkContext\n",
    "import pyspark.sql.functions as spf\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "#import streamlit as st\n",
    "import plotly as plt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Traitement ETL\n",
    "## CrÃ©ation d'un SparkSession"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "spark = SparkSession \\\n",
    "    .builder \\\n",
    "    .appName(\"how to read csv file\") \\\n",
    "    .getOrCreate()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Fonction d'ouverture de fichier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def openDataFrame(data):\n",
    "    df = spark.read.option(\"delimiter\", \",\") \\\n",
    "        .csv(f\"data/raw-file/{data}.csv\", header=True)\n",
    "    df.printSchema()\n",
    "    return df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Liste des fichiers Ã  traiter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "fileName = [\"dre_auvergne_rhone_alpes\",\n",
    "            \"dre_bourgogne_franche_comte\",\n",
    "            \"dre_bretagne\",\n",
    "            \"dre_centre_val_de_loire\",\n",
    "            \"dre_corse\",\n",
    "            \"dre_france_metropolitaine\",\n",
    "            \"dre_grand_est\",\n",
    "            \"dre_hauts_de_france\",\n",
    "            \"dre_ile_de_france\",\n",
    "            \"dre_normandie\",\n",
    "            \"dre_nouvelle_aquitaine\",\n",
    "            \"dre_occitanie\",\n",
    "            \"dre_pays_de_la_loire\",\n",
    "            \"dre_provence_alpes_cote_d_azur\",\n",
    "           ]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### EnlÃ¨ve la limitation de l'affichage du dataframe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "spark.conf.set('spark.sql.repl.eagerEval.enabled', True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Ouverture d'un fichier .csv et affichage de dataframe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- PRODUCTION: string (nullable = true)\n",
      " |-- _c1: string (nullable = true)\n",
      " |-- UnitÃ©: string (nullable = true)\n",
      " |-- 2014: string (nullable = true)\n",
      " |-- 2015: string (nullable = true)\n",
      " |-- 2016: string (nullable = true)\n",
      " |-- 2017: string (nullable = true)\n",
      " |-- 2018: string (nullable = true)\n",
      " |-- 2019: string (nullable = true)\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<table border='1'>\n",
       "<tr><th>PRODUCTION</th><th>_c1</th><th>UnitÃ©</th><th>2014</th><th>2015</th><th>2016</th><th>2017</th><th>2018</th><th>2019</th></tr>\n",
       "<tr><td>Production d&#x27;Ã©ner...</td><td>null</td><td>null</td><td>null</td><td>null</td><td>null</td><td>null</td><td>null</td><td>null</td></tr>\n",
       "<tr><td>P1</td><td>Production de cha...</td><td>kt</td><td>0</td><td>0</td><td>0</td><td>0</td><td>0</td><td>0</td></tr>\n",
       "<tr><td>P2</td><td>Production de pÃ©t...</td><td>kt</td><td>0</td><td>0</td><td>0</td><td>0</td><td>0</td><td>0</td></tr>\n",
       "<tr><td>P3</td><td>Production de gaz...</td><td>GWh PCS</td><td>0</td><td>0</td><td>0</td><td>0</td><td>0</td><td>0</td></tr>\n",
       "<tr><td>P4</td><td>Chaleur nuclÃ©aire...</td><td>GWh</td><td>266302</td><td>275368</td><td>227294</td><td>242398</td><td>243029</td><td>260090</td></tr>\n",
       "<tr><td>P5</td><td>Ã‰nergies renouvel...</td><td>GWh</td><td>30532</td><td>28268</td><td>30385</td><td>25659</td><td>32400</td><td>29961</td></tr>\n",
       "<tr><td>P7</td><td>Production totale...</td><td>ktep</td><td>0</td><td>0</td><td>0</td><td>0</td><td>0</td><td>0</td></tr>\n",
       "<tr><td>P8</td><td>  - production de...</td><td>ktep</td><td>0</td><td>0</td><td>0</td><td>0</td><td>0</td><td>0</td></tr>\n",
       "<tr><td>P9</td><td>  - production de...</td><td>ktep</td><td>0</td><td>0</td><td>0</td><td>0</td><td>0</td><td>0</td></tr>\n",
       "<tr><td>P10</td><td>  - production de...</td><td>ktep</td><td>0</td><td>0</td><td>0</td><td>0</td><td>0</td><td>0</td></tr>\n",
       "<tr><td>P11</td><td>Chaleur nuclÃ©aire...</td><td>ktep</td><td>22898</td><td>23677</td><td>19544</td><td>20842</td><td>20897</td><td>22364</td></tr>\n",
       "<tr><td>P12</td><td>Ã‰nergies renouvel...</td><td>ktep</td><td>2625</td><td>2431</td><td>2613</td><td>2206</td><td>2786</td><td>2576</td></tr>\n",
       "<tr><td>P13</td><td>BiomÃ©thane injectÃ©</td><td>GWh PCI</td><td>0</td><td>1</td><td>7</td><td>32</td><td>41</td><td>61</td></tr>\n",
       "<tr><td>P14</td><td>BiomÃ©thane inject...</td><td>ktep</td><td>0</td><td>0</td><td>1</td><td>3</td><td>4</td><td>5</td></tr>\n",
       "<tr><td>Ã‰lectricitÃ©</td><td>null</td><td>null</td><td>null</td><td>null</td><td>null</td><td>null</td><td>null</td><td>null</td></tr>\n",
       "<tr><td>Productions</td><td>null</td><td>null</td><td>null</td><td>null</td><td>null</td><td>null</td><td>null</td><td>null</td></tr>\n",
       "<tr><td>E1</td><td>Production totale...</td><td>GWh</td><td>120289</td><td>121896</td><td>108897</td><td>108896</td><td>115223</td><td>119056</td></tr>\n",
       "<tr><td>E2</td><td>Production d&#x27;Ã©lec...</td><td>GWh</td><td>30532</td><td>28268</td><td>30385</td><td>25659</td><td>32400</td><td>29961</td></tr>\n",
       "<tr><td>E4</td><td>  - Hydraulique (...</td><td>GWh</td><td>29â€¯120</td><td>26â€¯706</td><td>28â€¯708</td><td>23â€¯769</td><td>30â€¯280</td><td>27â€¯535</td></tr>\n",
       "<tr><td>E6</td><td>  - Ã‰olien </td><td>GWh</td><td>784</td><td>803</td><td>872</td><td>1â€¯006</td><td>1â€¯075</td><td>1â€¯194</td></tr>\n",
       "</table>\n",
       "only showing top 20 rows\n"
      ],
      "text/plain": [
       "+--------------------+--------------------+-------+------+------+------+------+------+------+\n",
       "|          PRODUCTION|                 _c1|  UnitÃ©|  2014|  2015|  2016|  2017|  2018|  2019|\n",
       "+--------------------+--------------------+-------+------+------+------+------+------+------+\n",
       "|Production d'Ã©ner...|                null|   null|  null|  null|  null|  null|  null|  null|\n",
       "|                  P1|Production de cha...|     kt|     0|     0|     0|     0|     0|     0|\n",
       "|                  P2|Production de pÃ©t...|     kt|     0|     0|     0|     0|     0|     0|\n",
       "|                  P3|Production de gaz...|GWh PCS|     0|     0|     0|     0|     0|     0|\n",
       "|                  P4|Chaleur nuclÃ©aire...|    GWh|266302|275368|227294|242398|243029|260090|\n",
       "|                  P5|Ã‰nergies renouvel...|    GWh| 30532| 28268| 30385| 25659| 32400| 29961|\n",
       "|                  P7|Production totale...|   ktep|     0|     0|     0|     0|     0|     0|\n",
       "|                  P8|  - production de...|   ktep|     0|     0|     0|     0|     0|     0|\n",
       "|                  P9|  - production de...|   ktep|     0|     0|     0|     0|     0|     0|\n",
       "|                 P10|  - production de...|   ktep|     0|     0|     0|     0|     0|     0|\n",
       "|                 P11|Chaleur nuclÃ©aire...|   ktep| 22898| 23677| 19544| 20842| 20897| 22364|\n",
       "|                 P12|Ã‰nergies renouvel...|   ktep|  2625|  2431|  2613|  2206|  2786|  2576|\n",
       "|                 P13|  BiomÃ©thane injectÃ©|GWh PCI|     0|     1|     7|    32|    41|    61|\n",
       "|                 P14|BiomÃ©thane inject...|   ktep|     0|     0|     1|     3|     4|     5|\n",
       "|         Ã‰lectricitÃ©|                null|   null|  null|  null|  null|  null|  null|  null|\n",
       "|         Productions|                null|   null|  null|  null|  null|  null|  null|  null|\n",
       "|                  E1|Production totale...|    GWh|120289|121896|108897|108896|115223|119056|\n",
       "|                  E2|Production d'Ã©lec...|    GWh| 30532| 28268| 30385| 25659| 32400| 29961|\n",
       "|                  E4|  - Hydraulique (...|    GWh|29â€¯120|26â€¯706|28â€¯708|23â€¯769|30â€¯280|27â€¯535|\n",
       "|                  E6|           - Ã‰olien |    GWh|   784|   803|   872| 1â€¯006| 1â€¯075| 1â€¯194|\n",
       "+--------------------+--------------------+-------+------+------+------+------+------+------+\n",
       "only showing top 20 rows"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = openDataFrame(fileName[0])\n",
    "df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Transposition de l'affichage"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-RECORD 0--------------------------\n",
      " PRODUCTION | Production d'Ã©ner... \n",
      " _c1        | null                 \n",
      " UnitÃ©      | null                 \n",
      " 2014       | null                 \n",
      " 2015       | null                 \n",
      " 2016       | null                 \n",
      " 2017       | null                 \n",
      " 2018       | null                 \n",
      " 2019       | null                 \n",
      "-RECORD 1--------------------------\n",
      " PRODUCTION | P1                   \n",
      " _c1        | Production de cha... \n",
      " UnitÃ©      | kt                   \n",
      " 2014       | 0                    \n",
      " 2015       | 0                    \n",
      " 2016       | 0                    \n",
      " 2017       | 0                    \n",
      " 2018       | 0                    \n",
      " 2019       | 0                    \n",
      "-RECORD 2--------------------------\n",
      " PRODUCTION | P2                   \n",
      " _c1        | Production de pÃ©t... \n",
      " UnitÃ©      | kt                   \n",
      " 2014       | 0                    \n",
      " 2015       | 0                    \n",
      " 2016       | 0                    \n",
      " 2017       | 0                    \n",
      " 2018       | 0                    \n",
      " 2019       | 0                    \n",
      "only showing top 3 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.show(3, vertical=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Affichage des colonnes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['PRODUCTION', '_c1', 'UnitÃ©', '2014', '2015', '2016', '2017', '2018', '2019']"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.columns"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Affichage des donnÃ©es"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- PRODUCTION: string (nullable = true)\n",
      " |-- _c1: string (nullable = true)\n",
      " |-- UnitÃ©: string (nullable = true)\n",
      " |-- 2014: string (nullable = true)\n",
      " |-- 2015: string (nullable = true)\n",
      " |-- 2016: string (nullable = true)\n",
      " |-- 2017: string (nullable = true)\n",
      " |-- 2018: string (nullable = true)\n",
      " |-- 2019: string (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.printSchema()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "La deuxiÃ¨me colonne ne porte pas de nom ..\n",
    "Nous allons y remÃ¨dier maintenant ðŸ˜‰"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<table border='1'>\n",
       "<tr><th>PRODUCTION</th><th>IntitulÃ© Ã©nergie</th><th>UnitÃ©</th><th>2014</th><th>2015</th><th>2016</th><th>2017</th><th>2018</th><th>2019</th></tr>\n",
       "<tr><td>Production d&#x27;Ã©ner...</td><td>null</td><td>null</td><td>null</td><td>null</td><td>null</td><td>null</td><td>null</td><td>null</td></tr>\n",
       "<tr><td>P1</td><td>Production de cha...</td><td>kt</td><td>0</td><td>0</td><td>0</td><td>0</td><td>0</td><td>0</td></tr>\n",
       "<tr><td>P2</td><td>Production de pÃ©t...</td><td>kt</td><td>0</td><td>0</td><td>0</td><td>0</td><td>0</td><td>0</td></tr>\n",
       "<tr><td>P3</td><td>Production de gaz...</td><td>GWh PCS</td><td>0</td><td>0</td><td>0</td><td>0</td><td>0</td><td>0</td></tr>\n",
       "<tr><td>P4</td><td>Chaleur nuclÃ©aire...</td><td>GWh</td><td>266302</td><td>275368</td><td>227294</td><td>242398</td><td>243029</td><td>260090</td></tr>\n",
       "<tr><td>P5</td><td>Ã‰nergies renouvel...</td><td>GWh</td><td>30532</td><td>28268</td><td>30385</td><td>25659</td><td>32400</td><td>29961</td></tr>\n",
       "<tr><td>P7</td><td>Production totale...</td><td>ktep</td><td>0</td><td>0</td><td>0</td><td>0</td><td>0</td><td>0</td></tr>\n",
       "<tr><td>P8</td><td>  - production de...</td><td>ktep</td><td>0</td><td>0</td><td>0</td><td>0</td><td>0</td><td>0</td></tr>\n",
       "<tr><td>P9</td><td>  - production de...</td><td>ktep</td><td>0</td><td>0</td><td>0</td><td>0</td><td>0</td><td>0</td></tr>\n",
       "<tr><td>P10</td><td>  - production de...</td><td>ktep</td><td>0</td><td>0</td><td>0</td><td>0</td><td>0</td><td>0</td></tr>\n",
       "<tr><td>P11</td><td>Chaleur nuclÃ©aire...</td><td>ktep</td><td>22898</td><td>23677</td><td>19544</td><td>20842</td><td>20897</td><td>22364</td></tr>\n",
       "<tr><td>P12</td><td>Ã‰nergies renouvel...</td><td>ktep</td><td>2625</td><td>2431</td><td>2613</td><td>2206</td><td>2786</td><td>2576</td></tr>\n",
       "<tr><td>P13</td><td>BiomÃ©thane injectÃ©</td><td>GWh PCI</td><td>0</td><td>1</td><td>7</td><td>32</td><td>41</td><td>61</td></tr>\n",
       "<tr><td>P14</td><td>BiomÃ©thane inject...</td><td>ktep</td><td>0</td><td>0</td><td>1</td><td>3</td><td>4</td><td>5</td></tr>\n",
       "<tr><td>Ã‰lectricitÃ©</td><td>null</td><td>null</td><td>null</td><td>null</td><td>null</td><td>null</td><td>null</td><td>null</td></tr>\n",
       "<tr><td>Productions</td><td>null</td><td>null</td><td>null</td><td>null</td><td>null</td><td>null</td><td>null</td><td>null</td></tr>\n",
       "<tr><td>E1</td><td>Production totale...</td><td>GWh</td><td>120289</td><td>121896</td><td>108897</td><td>108896</td><td>115223</td><td>119056</td></tr>\n",
       "<tr><td>E2</td><td>Production d&#x27;Ã©lec...</td><td>GWh</td><td>30532</td><td>28268</td><td>30385</td><td>25659</td><td>32400</td><td>29961</td></tr>\n",
       "<tr><td>E4</td><td>  - Hydraulique (...</td><td>GWh</td><td>29â€¯120</td><td>26â€¯706</td><td>28â€¯708</td><td>23â€¯769</td><td>30â€¯280</td><td>27â€¯535</td></tr>\n",
       "<tr><td>E6</td><td>  - Ã‰olien </td><td>GWh</td><td>784</td><td>803</td><td>872</td><td>1â€¯006</td><td>1â€¯075</td><td>1â€¯194</td></tr>\n",
       "</table>\n",
       "only showing top 20 rows\n"
      ],
      "text/plain": [
       "+--------------------+--------------------+-------+------+------+------+------+------+------+\n",
       "|          PRODUCTION|    IntitulÃ© Ã©nergie|  UnitÃ©|  2014|  2015|  2016|  2017|  2018|  2019|\n",
       "+--------------------+--------------------+-------+------+------+------+------+------+------+\n",
       "|Production d'Ã©ner...|                null|   null|  null|  null|  null|  null|  null|  null|\n",
       "|                  P1|Production de cha...|     kt|     0|     0|     0|     0|     0|     0|\n",
       "|                  P2|Production de pÃ©t...|     kt|     0|     0|     0|     0|     0|     0|\n",
       "|                  P3|Production de gaz...|GWh PCS|     0|     0|     0|     0|     0|     0|\n",
       "|                  P4|Chaleur nuclÃ©aire...|    GWh|266302|275368|227294|242398|243029|260090|\n",
       "|                  P5|Ã‰nergies renouvel...|    GWh| 30532| 28268| 30385| 25659| 32400| 29961|\n",
       "|                  P7|Production totale...|   ktep|     0|     0|     0|     0|     0|     0|\n",
       "|                  P8|  - production de...|   ktep|     0|     0|     0|     0|     0|     0|\n",
       "|                  P9|  - production de...|   ktep|     0|     0|     0|     0|     0|     0|\n",
       "|                 P10|  - production de...|   ktep|     0|     0|     0|     0|     0|     0|\n",
       "|                 P11|Chaleur nuclÃ©aire...|   ktep| 22898| 23677| 19544| 20842| 20897| 22364|\n",
       "|                 P12|Ã‰nergies renouvel...|   ktep|  2625|  2431|  2613|  2206|  2786|  2576|\n",
       "|                 P13|  BiomÃ©thane injectÃ©|GWh PCI|     0|     1|     7|    32|    41|    61|\n",
       "|                 P14|BiomÃ©thane inject...|   ktep|     0|     0|     1|     3|     4|     5|\n",
       "|         Ã‰lectricitÃ©|                null|   null|  null|  null|  null|  null|  null|  null|\n",
       "|         Productions|                null|   null|  null|  null|  null|  null|  null|  null|\n",
       "|                  E1|Production totale...|    GWh|120289|121896|108897|108896|115223|119056|\n",
       "|                  E2|Production d'Ã©lec...|    GWh| 30532| 28268| 30385| 25659| 32400| 29961|\n",
       "|                  E4|  - Hydraulique (...|    GWh|29â€¯120|26â€¯706|28â€¯708|23â€¯769|30â€¯280|27â€¯535|\n",
       "|                  E6|           - Ã‰olien |    GWh|   784|   803|   872| 1â€¯006| 1â€¯075| 1â€¯194|\n",
       "+--------------------+--------------------+-------+------+------+------+------+------+------+\n",
       "only showing top 20 rows"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# def withColumnRenamed(existingName: String, newName: String): DataFrame\n",
    "df = df.withColumnRenamed(\"_c1\",\"IntitulÃ© Ã©nergie\")\n",
    "df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Sommaire des donnÃ©es"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+-----------------+-----------------+\n",
      "|summary|             2014|             2015|\n",
      "+-------+-----------------+-----------------+\n",
      "|  count|              123|              123|\n",
      "|   mean|7200.949367088608|7521.220779220779|\n",
      "| stddev|32936.38346909151|34321.52734326226|\n",
      "|    min|                0|                0|\n",
      "|    max|  Secret primaire|  Secret primaire|\n",
      "+-------+-----------------+-----------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.select('2014', '2015').describe().show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "# PermiÃ¨re version des liste\n",
    "\n",
    "list_production = [\n",
    "    'P1','P2','P3','P4','P5','P7','P8','P9','P10','P11','P12','P13','P14',\n",
    "    'E1','E2','E4','E6','E7','E8','E9','E10','E11','E12','E13','E14','E15','E16','E17','E18','E19','E20','E21','E22','E23','E24','E25','E26','E27','E28'\n",
    "]\n",
    "\n",
    "list_consomation = [\n",
    "    'C1','CI1','CI3','CI3b','CI4','CI9','CI11','CI5','CI6','CI7','CI8',\n",
    "    'CTR1','CTR2','CTR3','CTR4','CTR5','CTR6','CTR7','CTR8','CTR9','CTR10','CTR11',\n",
    "    'CR1','CR7','CR2','CR3','CR4','CR5','CR6','CR8',\n",
    "    'CTE1','CTE7','CTE2','CTE3','CTE4','CTE5','CTE6','CTE8',\n",
    "    'CA1','CA7','CA2','CA21','CA3','CA4','CA5','CA6','CA8',\n",
    "    'C2','C3','C4','C13','C14'\n",
    "]\n",
    "\n",
    "# Penser Ã  passer par une tranposition puis pour un passae d'indexe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+--------------------+-------+------+------+------+------+------+------+\n",
      "|PRODUCTION|    IntitulÃ© Ã©nergie|  UnitÃ©|  2014|  2015|  2016|  2017|  2018|  2019|\n",
      "+----------+--------------------+-------+------+------+------+------+------+------+\n",
      "|        P1|Production de cha...|     kt|     0|     0|     0|     0|     0|     0|\n",
      "|        P2|Production de pÃ©t...|     kt|     0|     0|     0|     0|     0|     0|\n",
      "|        P3|Production de gaz...|GWh PCS|     0|     0|     0|     0|     0|     0|\n",
      "|        P4|Chaleur nuclÃ©aire...|    GWh|266302|275368|227294|242398|243029|260090|\n",
      "|        P5|Ã‰nergies renouvel...|    GWh| 30532| 28268| 30385| 25659| 32400| 29961|\n",
      "|        P7|Production totale...|   ktep|     0|     0|     0|     0|     0|     0|\n",
      "|        P8|  - production de...|   ktep|     0|     0|     0|     0|     0|     0|\n",
      "|        P9|  - production de...|   ktep|     0|     0|     0|     0|     0|     0|\n",
      "|       P10|  - production de...|   ktep|     0|     0|     0|     0|     0|     0|\n",
      "|       P11|Chaleur nuclÃ©aire...|   ktep| 22898| 23677| 19544| 20842| 20897| 22364|\n",
      "|       P12|Ã‰nergies renouvel...|   ktep|  2625|  2431|  2613|  2206|  2786|  2576|\n",
      "|       P13|  BiomÃ©thane injectÃ©|GWh PCI|     0|     1|     7|    32|    41|    61|\n",
      "|       P14|BiomÃ©thane inject...|   ktep|     0|     0|     1|     3|     4|     5|\n",
      "|        E1|Production totale...|    GWh|120289|121896|108897|108896|115223|119056|\n",
      "|        E2|Production d'Ã©lec...|    GWh| 30532| 28268| 30385| 25659| 32400| 29961|\n",
      "|        E4|  - Hydraulique (...|    GWh|29â€¯120|26â€¯706|28â€¯708|23â€¯769|30â€¯280|27â€¯535|\n",
      "|        E6|           - Ã‰olien |    GWh|   784|   803|   872| 1â€¯006| 1â€¯075| 1â€¯194|\n",
      "|        E7|  - Solaire photo...|    GWh|   627|   759|   805|   883| 1â€¯045| 1â€¯232|\n",
      "|        E8|Production d'Ã©lec...|    GWh|87â€¯880|90â€¯871|75â€¯007|79â€¯991|80â€¯199|85â€¯830|\n",
      "|        E9|Production nette ...|    GWh|  1877|  2756|  3506|  3247|  2624|  3265|\n",
      "+----------+--------------------+-------+------+------+------+------+------+------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df_production = df.filter(df.PRODUCTION.isin(list_production))\n",
    "df_production.show()\n",
    "\n",
    "## Liste de test non concluant\n",
    "#subset_df = df.filter(df.PRODUCTION.isin(list_production))\n",
    "#df_production = spark.createDataFrame(subset_df, df.columns)\n",
    "#df_production = df.withColumn(df.columns[0], spf.when(df.PRODUCTION.isin(list_production)))\n",
    "#df_production.show"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+--------------------+-------+------+------+------+------+------+------+\n",
      "|PRODUCTION|    IntitulÃ© Ã©nergie|  UnitÃ©|  2014|  2015|  2016|  2017|  2018|  2019|\n",
      "+----------+--------------------+-------+------+------+------+------+------+------+\n",
      "|        C1|Consommation fina...|   ktep| 17948| 18567| 18559| 18746| 18327| 18369|\n",
      "|       CI1|           Industrie|   ktep|  3802|  3860|  3971|  3930|  3826|  3909|\n",
      "|       CI3|             Charbon|   ktep|    60|    61|    63|   112|   114|   112|\n",
      "|      CI3b|dont hauts fourneaux|   ktep|     0|     0|     0|     0|     0|     0|\n",
      "|       CI4| Produits pÃ©troliers|   ktep|   387|   356|   366|   362|   347|   350|\n",
      "|       CI9|Energies renouvel...|   ktep|   156|   154|   164|   148|   160|   169|\n",
      "|      CI11|Chaleur commercia...|   ktep|   215|   236|   231|   222|   181|   258|\n",
      "|       CI5|Gaz naturel (unit...|GWh PCS|16â€¯969|17â€¯484|18â€¯511|17â€¯338|16â€¯490|16â€¯819|\n",
      "|       CI6|         Gaz naturel|   ktep| 1â€¯313| 1â€¯353| 1â€¯432| 1â€¯342| 1â€¯276| 1â€¯302|\n",
      "|       CI7|Ã‰lectricitÃ© (unit...|    GWh|19â€¯446|19â€¯767|19â€¯946|20â€¯280|20â€¯337|19â€¯971|\n",
      "|       CI8|        Ã‰lectricitÃ© |   ktep| 1â€¯672| 1â€¯700| 1â€¯715| 1â€¯744| 1â€¯749| 1â€¯717|\n",
      "|      CTR1|           Transport|   ktep|  6282|  6414|  6288|  6408|  6359|  6382|\n",
      "|      CTR2|Produits pÃ©trolie...|   ktep| 5â€¯765| 5â€¯883| 5â€¯769| 5â€¯864| 5â€¯815| 5â€¯821|\n",
      "|      CTR3|       Biocarburants|   ktep|   400|   409|   400|   423|   427|   438|\n",
      "|      CTR4|Supercarburants (...|   ktep|   936|   972| 1â€¯000| 1â€¯049| 1â€¯101| 1â€¯190|\n",
      "|      CTR5|Gazole routier (l...|   ktep| 4â€¯704| 4â€¯782| 4â€¯656| 4â€¯697| 4â€¯572| 4â€¯516|\n",
      "|      CTR6|Biocarburants ess...|   ktep|    53|    57|    62|    71|    78|    88|\n",
      "|      CTR7|Biocarburants gazole|   ktep|   347|   352|   337|   350|   347|   348|\n",
      "|      CTR8|Gaz naturel (unit...|GWh PCS|    73|    75|    77|    84|   127|   162|\n",
      "|      CTR9|         Gaz naturel|   ktep|     6|     6|     6|     6|    10|    13|\n",
      "+----------+--------------------+-------+------+------+------+------+------+------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df_consomation = df.filter(df.PRODUCTION.isin(list_consomation))\n",
    "df_consomation.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Sauvegardes \n",
    "\n",
    "###  /!\\ En Chantier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [],
   "source": [
    "def saveDataFrame(dataframe, file_name):\n",
    "    dataframe.write.csv(f\"data/refine-file/{file_name}.csv\", header=True)\n",
    "    return \"file_name as been succesfully create\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "ename": "Py4JJavaError",
     "evalue": "An error occurred while calling o993.csv.\n: org.apache.spark.SparkException: Job aborted.\r\n\tat org.apache.spark.sql.execution.datasources.FileFormatWriter$.write(FileFormatWriter.scala:231)\r\n\tat org.apache.spark.sql.execution.datasources.InsertIntoHadoopFsRelationCommand.run(InsertIntoHadoopFsRelationCommand.scala:188)\r\n\tat org.apache.spark.sql.execution.command.DataWritingCommandExec.sideEffectResult$lzycompute(commands.scala:108)\r\n\tat org.apache.spark.sql.execution.command.DataWritingCommandExec.sideEffectResult(commands.scala:106)\r\n\tat org.apache.spark.sql.execution.command.DataWritingCommandExec.doExecute(commands.scala:131)\r\n\tat org.apache.spark.sql.execution.SparkPlan.$anonfun$execute$1(SparkPlan.scala:180)\r\n\tat org.apache.spark.sql.execution.SparkPlan.$anonfun$executeQuery$1(SparkPlan.scala:218)\r\n\tat org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:151)\r\n\tat org.apache.spark.sql.execution.SparkPlan.executeQuery(SparkPlan.scala:215)\r\n\tat org.apache.spark.sql.execution.SparkPlan.execute(SparkPlan.scala:176)\r\n\tat org.apache.spark.sql.execution.QueryExecution.toRdd$lzycompute(QueryExecution.scala:132)\r\n\tat org.apache.spark.sql.execution.QueryExecution.toRdd(QueryExecution.scala:131)\r\n\tat org.apache.spark.sql.DataFrameWriter.$anonfun$runCommand$1(DataFrameWriter.scala:989)\r\n\tat org.apache.spark.sql.execution.SQLExecution$.$anonfun$withNewExecutionId$5(SQLExecution.scala:103)\r\n\tat org.apache.spark.sql.execution.SQLExecution$.withSQLConfPropagated(SQLExecution.scala:163)\r\n\tat org.apache.spark.sql.execution.SQLExecution$.$anonfun$withNewExecutionId$1(SQLExecution.scala:90)\r\n\tat org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:772)\r\n\tat org.apache.spark.sql.execution.SQLExecution$.withNewExecutionId(SQLExecution.scala:64)\r\n\tat org.apache.spark.sql.DataFrameWriter.runCommand(DataFrameWriter.scala:989)\r\n\tat org.apache.spark.sql.DataFrameWriter.saveToV1Source(DataFrameWriter.scala:438)\r\n\tat org.apache.spark.sql.DataFrameWriter.saveInternal(DataFrameWriter.scala:415)\r\n\tat org.apache.spark.sql.DataFrameWriter.save(DataFrameWriter.scala:293)\r\n\tat org.apache.spark.sql.DataFrameWriter.csv(DataFrameWriter.scala:979)\r\n\tat sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)\r\n\tat sun.reflect.NativeMethodAccessorImpl.invoke(Unknown Source)\r\n\tat sun.reflect.DelegatingMethodAccessorImpl.invoke(Unknown Source)\r\n\tat java.lang.reflect.Method.invoke(Unknown Source)\r\n\tat py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244)\r\n\tat py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:357)\r\n\tat py4j.Gateway.invoke(Gateway.java:282)\r\n\tat py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132)\r\n\tat py4j.commands.CallCommand.execute(CallCommand.java:79)\r\n\tat py4j.GatewayConnection.run(GatewayConnection.java:238)\r\n\tat java.lang.Thread.run(Unknown Source)\r\nCaused by: org.apache.spark.SparkException: Job aborted due to stage failure: Task 0 in stage 25.0 failed 1 times, most recent failure: Lost task 0.0 in stage 25.0 (TID 25) (DESKTOP-8KB7N7C.mshome.net executor driver): java.io.IOException: Mkdirs failed to create file:/C:/Users/Antoine/Documents/spark_nrg-master/data/refine-file/production_['dre_auvergne_rhone_alpes', 'dre_bourgogne_franche_comte', 'dre_bretagne', 'dre_centre_val_de_loire', 'dre_corse', 'dre_france_metropolitaine', 'dre_grand_est', 'dre_hauts_de_france', 'dre_ile_de_france', 'dre_normandie', 'dre_nouvelle_aquitaine', 'dre_occitanie', 'dre_pays_de_la_loire', 'dre_provence_alpes_cote_d_azur'].csv/_temporary/0/_temporary/attempt_202202040111283260659664240352794_0025_m_000000_25 (exists=false, cwd=file:/C:/Users/Antoine/Documents/spark_nrg-master)\r\n\tat org.apache.hadoop.fs.ChecksumFileSystem.create(ChecksumFileSystem.java:458)\r\n\tat org.apache.hadoop.fs.ChecksumFileSystem.create(ChecksumFileSystem.java:443)\r\n\tat org.apache.hadoop.fs.FileSystem.create(FileSystem.java:1118)\r\n\tat org.apache.hadoop.fs.FileSystem.create(FileSystem.java:1098)\r\n\tat org.apache.hadoop.fs.FileSystem.create(FileSystem.java:987)\r\n\tat org.apache.spark.sql.execution.datasources.CodecStreams$.createOutputStream(CodecStreams.scala:81)\r\n\tat org.apache.spark.sql.execution.datasources.CodecStreams$.createOutputStreamWriter(CodecStreams.scala:92)\r\n\tat org.apache.spark.sql.execution.datasources.csv.CsvOutputWriter.<init>(CsvOutputWriter.scala:38)\r\n\tat org.apache.spark.sql.execution.datasources.csv.CSVFileFormat$$anon$1.newInstance(CSVFileFormat.scala:84)\r\n\tat org.apache.spark.sql.execution.datasources.SingleDirectoryDataWriter.newOutputWriter(FileFormatDataWriter.scala:126)\r\n\tat org.apache.spark.sql.execution.datasources.SingleDirectoryDataWriter.<init>(FileFormatDataWriter.scala:111)\r\n\tat org.apache.spark.sql.execution.datasources.FileFormatWriter$.executeTask(FileFormatWriter.scala:269)\r\n\tat org.apache.spark.sql.execution.datasources.FileFormatWriter$.$anonfun$write$15(FileFormatWriter.scala:210)\r\n\tat org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:90)\r\n\tat org.apache.spark.scheduler.Task.run(Task.scala:131)\r\n\tat org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$3(Executor.scala:497)\r\n\tat org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:1439)\r\n\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:500)\r\n\tat java.util.concurrent.ThreadPoolExecutor.runWorker(Unknown Source)\r\n\tat java.util.concurrent.ThreadPoolExecutor$Worker.run(Unknown Source)\r\n\tat java.lang.Thread.run(Unknown Source)\r\n\nDriver stacktrace:\r\n\tat org.apache.spark.scheduler.DAGScheduler.failJobAndIndependentStages(DAGScheduler.scala:2253)\r\n\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$abortStage$2(DAGScheduler.scala:2202)\r\n\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$abortStage$2$adapted(DAGScheduler.scala:2201)\r\n\tat scala.collection.mutable.ResizableArray.foreach(ResizableArray.scala:62)\r\n\tat scala.collection.mutable.ResizableArray.foreach$(ResizableArray.scala:55)\r\n\tat scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:49)\r\n\tat org.apache.spark.scheduler.DAGScheduler.abortStage(DAGScheduler.scala:2201)\r\n\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$handleTaskSetFailed$1(DAGScheduler.scala:1078)\r\n\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$handleTaskSetFailed$1$adapted(DAGScheduler.scala:1078)\r\n\tat scala.Option.foreach(Option.scala:407)\r\n\tat org.apache.spark.scheduler.DAGScheduler.handleTaskSetFailed(DAGScheduler.scala:1078)\r\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.doOnReceive(DAGScheduler.scala:2440)\r\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:2382)\r\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:2371)\r\n\tat org.apache.spark.util.EventLoop$$anon$1.run(EventLoop.scala:49)\r\n\tat org.apache.spark.scheduler.DAGScheduler.runJob(DAGScheduler.scala:868)\r\n\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:2202)\r\n\tat org.apache.spark.sql.execution.datasources.FileFormatWriter$.write(FileFormatWriter.scala:200)\r\n\t... 33 more\r\nCaused by: java.io.IOException: Mkdirs failed to create file:/C:/Users/Antoine/Documents/spark_nrg-master/data/refine-file/production_['dre_auvergne_rhone_alpes', 'dre_bourgogne_franche_comte', 'dre_bretagne', 'dre_centre_val_de_loire', 'dre_corse', 'dre_france_metropolitaine', 'dre_grand_est', 'dre_hauts_de_france', 'dre_ile_de_france', 'dre_normandie', 'dre_nouvelle_aquitaine', 'dre_occitanie', 'dre_pays_de_la_loire', 'dre_provence_alpes_cote_d_azur'].csv/_temporary/0/_temporary/attempt_202202040111283260659664240352794_0025_m_000000_25 (exists=false, cwd=file:/C:/Users/Antoine/Documents/spark_nrg-master)\r\n\tat org.apache.hadoop.fs.ChecksumFileSystem.create(ChecksumFileSystem.java:458)\r\n\tat org.apache.hadoop.fs.ChecksumFileSystem.create(ChecksumFileSystem.java:443)\r\n\tat org.apache.hadoop.fs.FileSystem.create(FileSystem.java:1118)\r\n\tat org.apache.hadoop.fs.FileSystem.create(FileSystem.java:1098)\r\n\tat org.apache.hadoop.fs.FileSystem.create(FileSystem.java:987)\r\n\tat org.apache.spark.sql.execution.datasources.CodecStreams$.createOutputStream(CodecStreams.scala:81)\r\n\tat org.apache.spark.sql.execution.datasources.CodecStreams$.createOutputStreamWriter(CodecStreams.scala:92)\r\n\tat org.apache.spark.sql.execution.datasources.csv.CsvOutputWriter.<init>(CsvOutputWriter.scala:38)\r\n\tat org.apache.spark.sql.execution.datasources.csv.CSVFileFormat$$anon$1.newInstance(CSVFileFormat.scala:84)\r\n\tat org.apache.spark.sql.execution.datasources.SingleDirectoryDataWriter.newOutputWriter(FileFormatDataWriter.scala:126)\r\n\tat org.apache.spark.sql.execution.datasources.SingleDirectoryDataWriter.<init>(FileFormatDataWriter.scala:111)\r\n\tat org.apache.spark.sql.execution.datasources.FileFormatWriter$.executeTask(FileFormatWriter.scala:269)\r\n\tat org.apache.spark.sql.execution.datasources.FileFormatWriter$.$anonfun$write$15(FileFormatWriter.scala:210)\r\n\tat org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:90)\r\n\tat org.apache.spark.scheduler.Task.run(Task.scala:131)\r\n\tat org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$3(Executor.scala:497)\r\n\tat org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:1439)\r\n\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:500)\r\n\tat java.util.concurrent.ThreadPoolExecutor.runWorker(Unknown Source)\r\n\tat java.util.concurrent.ThreadPoolExecutor$Worker.run(Unknown Source)\r\n\t... 1 more\r\n",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mPy4JJavaError\u001b[0m                             Traceback (most recent call last)",
      "\u001b[1;32m~\\AppData\\Local\\Temp/ipykernel_9908/703917499.py\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[0msaveDataFrame\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdf_production\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34mf\"production_{fileName}\"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;32m~\\AppData\\Local\\Temp/ipykernel_9908/3763759178.py\u001b[0m in \u001b[0;36msaveDataFrame\u001b[1;34m(dataframe, file_name)\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[1;32mdef\u001b[0m \u001b[0msaveDataFrame\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdataframe\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfile_name\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 2\u001b[1;33m     \u001b[0mdataframe\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mwrite\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcsv\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34mf\"data/refine-file/{file_name}.csv\"\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mheader\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mTrue\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      3\u001b[0m     \u001b[1;32mreturn\u001b[0m \u001b[1;34m\"file_name as been succesfully create\"\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\users\\antoine\\appdata\\local\\programs\\python\\python38-32\\lib\\site-packages\\pyspark\\sql\\readwriter.py\u001b[0m in \u001b[0;36mcsv\u001b[1;34m(self, path, mode, compression, sep, quote, escape, header, nullValue, escapeQuotes, quoteAll, dateFormat, timestampFormat, ignoreLeadingWhiteSpace, ignoreTrailingWhiteSpace, charToEscapeQuoteEscaping, encoding, emptyValue, lineSep)\u001b[0m\n\u001b[0;32m   1369\u001b[0m                        \u001b[0mcharToEscapeQuoteEscaping\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mcharToEscapeQuoteEscaping\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1370\u001b[0m                        encoding=encoding, emptyValue=emptyValue, lineSep=lineSep)\n\u001b[1;32m-> 1371\u001b[1;33m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_jwrite\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcsv\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mpath\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1372\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1373\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0morc\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mpath\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mmode\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mNone\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mpartitionBy\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mNone\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mcompression\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mNone\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\users\\antoine\\appdata\\local\\programs\\python\\python38-32\\lib\\site-packages\\py4j\\java_gateway.py\u001b[0m in \u001b[0;36m__call__\u001b[1;34m(self, *args)\u001b[0m\n\u001b[0;32m   1302\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1303\u001b[0m         \u001b[0manswer\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mgateway_client\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msend_command\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mcommand\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1304\u001b[1;33m         return_value = get_return_value(\n\u001b[0m\u001b[0;32m   1305\u001b[0m             answer, self.gateway_client, self.target_id, self.name)\n\u001b[0;32m   1306\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\users\\antoine\\appdata\\local\\programs\\python\\python38-32\\lib\\site-packages\\pyspark\\sql\\utils.py\u001b[0m in \u001b[0;36mdeco\u001b[1;34m(*a, **kw)\u001b[0m\n\u001b[0;32m    109\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0mdeco\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0ma\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkw\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    110\u001b[0m         \u001b[1;32mtry\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 111\u001b[1;33m             \u001b[1;32mreturn\u001b[0m \u001b[0mf\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0ma\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkw\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    112\u001b[0m         \u001b[1;32mexcept\u001b[0m \u001b[0mpy4j\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mprotocol\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mPy4JJavaError\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    113\u001b[0m             \u001b[0mconverted\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mconvert_exception\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0me\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mjava_exception\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\users\\antoine\\appdata\\local\\programs\\python\\python38-32\\lib\\site-packages\\py4j\\protocol.py\u001b[0m in \u001b[0;36mget_return_value\u001b[1;34m(answer, gateway_client, target_id, name)\u001b[0m\n\u001b[0;32m    324\u001b[0m             \u001b[0mvalue\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mOUTPUT_CONVERTER\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mtype\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0manswer\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m2\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mgateway_client\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    325\u001b[0m             \u001b[1;32mif\u001b[0m \u001b[0manswer\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m==\u001b[0m \u001b[0mREFERENCE_TYPE\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 326\u001b[1;33m                 raise Py4JJavaError(\n\u001b[0m\u001b[0;32m    327\u001b[0m                     \u001b[1;34m\"An error occurred while calling {0}{1}{2}.\\n\"\u001b[0m\u001b[1;33m.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    328\u001b[0m                     format(target_id, \".\", name), value)\n",
      "\u001b[1;31mPy4JJavaError\u001b[0m: An error occurred while calling o993.csv.\n: org.apache.spark.SparkException: Job aborted.\r\n\tat org.apache.spark.sql.execution.datasources.FileFormatWriter$.write(FileFormatWriter.scala:231)\r\n\tat org.apache.spark.sql.execution.datasources.InsertIntoHadoopFsRelationCommand.run(InsertIntoHadoopFsRelationCommand.scala:188)\r\n\tat org.apache.spark.sql.execution.command.DataWritingCommandExec.sideEffectResult$lzycompute(commands.scala:108)\r\n\tat org.apache.spark.sql.execution.command.DataWritingCommandExec.sideEffectResult(commands.scala:106)\r\n\tat org.apache.spark.sql.execution.command.DataWritingCommandExec.doExecute(commands.scala:131)\r\n\tat org.apache.spark.sql.execution.SparkPlan.$anonfun$execute$1(SparkPlan.scala:180)\r\n\tat org.apache.spark.sql.execution.SparkPlan.$anonfun$executeQuery$1(SparkPlan.scala:218)\r\n\tat org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:151)\r\n\tat org.apache.spark.sql.execution.SparkPlan.executeQuery(SparkPlan.scala:215)\r\n\tat org.apache.spark.sql.execution.SparkPlan.execute(SparkPlan.scala:176)\r\n\tat org.apache.spark.sql.execution.QueryExecution.toRdd$lzycompute(QueryExecution.scala:132)\r\n\tat org.apache.spark.sql.execution.QueryExecution.toRdd(QueryExecution.scala:131)\r\n\tat org.apache.spark.sql.DataFrameWriter.$anonfun$runCommand$1(DataFrameWriter.scala:989)\r\n\tat org.apache.spark.sql.execution.SQLExecution$.$anonfun$withNewExecutionId$5(SQLExecution.scala:103)\r\n\tat org.apache.spark.sql.execution.SQLExecution$.withSQLConfPropagated(SQLExecution.scala:163)\r\n\tat org.apache.spark.sql.execution.SQLExecution$.$anonfun$withNewExecutionId$1(SQLExecution.scala:90)\r\n\tat org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:772)\r\n\tat org.apache.spark.sql.execution.SQLExecution$.withNewExecutionId(SQLExecution.scala:64)\r\n\tat org.apache.spark.sql.DataFrameWriter.runCommand(DataFrameWriter.scala:989)\r\n\tat org.apache.spark.sql.DataFrameWriter.saveToV1Source(DataFrameWriter.scala:438)\r\n\tat org.apache.spark.sql.DataFrameWriter.saveInternal(DataFrameWriter.scala:415)\r\n\tat org.apache.spark.sql.DataFrameWriter.save(DataFrameWriter.scala:293)\r\n\tat org.apache.spark.sql.DataFrameWriter.csv(DataFrameWriter.scala:979)\r\n\tat sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)\r\n\tat sun.reflect.NativeMethodAccessorImpl.invoke(Unknown Source)\r\n\tat sun.reflect.DelegatingMethodAccessorImpl.invoke(Unknown Source)\r\n\tat java.lang.reflect.Method.invoke(Unknown Source)\r\n\tat py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244)\r\n\tat py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:357)\r\n\tat py4j.Gateway.invoke(Gateway.java:282)\r\n\tat py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132)\r\n\tat py4j.commands.CallCommand.execute(CallCommand.java:79)\r\n\tat py4j.GatewayConnection.run(GatewayConnection.java:238)\r\n\tat java.lang.Thread.run(Unknown Source)\r\nCaused by: org.apache.spark.SparkException: Job aborted due to stage failure: Task 0 in stage 25.0 failed 1 times, most recent failure: Lost task 0.0 in stage 25.0 (TID 25) (DESKTOP-8KB7N7C.mshome.net executor driver): java.io.IOException: Mkdirs failed to create file:/C:/Users/Antoine/Documents/spark_nrg-master/data/refine-file/production_['dre_auvergne_rhone_alpes', 'dre_bourgogne_franche_comte', 'dre_bretagne', 'dre_centre_val_de_loire', 'dre_corse', 'dre_france_metropolitaine', 'dre_grand_est', 'dre_hauts_de_france', 'dre_ile_de_france', 'dre_normandie', 'dre_nouvelle_aquitaine', 'dre_occitanie', 'dre_pays_de_la_loire', 'dre_provence_alpes_cote_d_azur'].csv/_temporary/0/_temporary/attempt_202202040111283260659664240352794_0025_m_000000_25 (exists=false, cwd=file:/C:/Users/Antoine/Documents/spark_nrg-master)\r\n\tat org.apache.hadoop.fs.ChecksumFileSystem.create(ChecksumFileSystem.java:458)\r\n\tat org.apache.hadoop.fs.ChecksumFileSystem.create(ChecksumFileSystem.java:443)\r\n\tat org.apache.hadoop.fs.FileSystem.create(FileSystem.java:1118)\r\n\tat org.apache.hadoop.fs.FileSystem.create(FileSystem.java:1098)\r\n\tat org.apache.hadoop.fs.FileSystem.create(FileSystem.java:987)\r\n\tat org.apache.spark.sql.execution.datasources.CodecStreams$.createOutputStream(CodecStreams.scala:81)\r\n\tat org.apache.spark.sql.execution.datasources.CodecStreams$.createOutputStreamWriter(CodecStreams.scala:92)\r\n\tat org.apache.spark.sql.execution.datasources.csv.CsvOutputWriter.<init>(CsvOutputWriter.scala:38)\r\n\tat org.apache.spark.sql.execution.datasources.csv.CSVFileFormat$$anon$1.newInstance(CSVFileFormat.scala:84)\r\n\tat org.apache.spark.sql.execution.datasources.SingleDirectoryDataWriter.newOutputWriter(FileFormatDataWriter.scala:126)\r\n\tat org.apache.spark.sql.execution.datasources.SingleDirectoryDataWriter.<init>(FileFormatDataWriter.scala:111)\r\n\tat org.apache.spark.sql.execution.datasources.FileFormatWriter$.executeTask(FileFormatWriter.scala:269)\r\n\tat org.apache.spark.sql.execution.datasources.FileFormatWriter$.$anonfun$write$15(FileFormatWriter.scala:210)\r\n\tat org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:90)\r\n\tat org.apache.spark.scheduler.Task.run(Task.scala:131)\r\n\tat org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$3(Executor.scala:497)\r\n\tat org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:1439)\r\n\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:500)\r\n\tat java.util.concurrent.ThreadPoolExecutor.runWorker(Unknown Source)\r\n\tat java.util.concurrent.ThreadPoolExecutor$Worker.run(Unknown Source)\r\n\tat java.lang.Thread.run(Unknown Source)\r\n\nDriver stacktrace:\r\n\tat org.apache.spark.scheduler.DAGScheduler.failJobAndIndependentStages(DAGScheduler.scala:2253)\r\n\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$abortStage$2(DAGScheduler.scala:2202)\r\n\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$abortStage$2$adapted(DAGScheduler.scala:2201)\r\n\tat scala.collection.mutable.ResizableArray.foreach(ResizableArray.scala:62)\r\n\tat scala.collection.mutable.ResizableArray.foreach$(ResizableArray.scala:55)\r\n\tat scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:49)\r\n\tat org.apache.spark.scheduler.DAGScheduler.abortStage(DAGScheduler.scala:2201)\r\n\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$handleTaskSetFailed$1(DAGScheduler.scala:1078)\r\n\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$handleTaskSetFailed$1$adapted(DAGScheduler.scala:1078)\r\n\tat scala.Option.foreach(Option.scala:407)\r\n\tat org.apache.spark.scheduler.DAGScheduler.handleTaskSetFailed(DAGScheduler.scala:1078)\r\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.doOnReceive(DAGScheduler.scala:2440)\r\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:2382)\r\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:2371)\r\n\tat org.apache.spark.util.EventLoop$$anon$1.run(EventLoop.scala:49)\r\n\tat org.apache.spark.scheduler.DAGScheduler.runJob(DAGScheduler.scala:868)\r\n\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:2202)\r\n\tat org.apache.spark.sql.execution.datasources.FileFormatWriter$.write(FileFormatWriter.scala:200)\r\n\t... 33 more\r\nCaused by: java.io.IOException: Mkdirs failed to create file:/C:/Users/Antoine/Documents/spark_nrg-master/data/refine-file/production_['dre_auvergne_rhone_alpes', 'dre_bourgogne_franche_comte', 'dre_bretagne', 'dre_centre_val_de_loire', 'dre_corse', 'dre_france_metropolitaine', 'dre_grand_est', 'dre_hauts_de_france', 'dre_ile_de_france', 'dre_normandie', 'dre_nouvelle_aquitaine', 'dre_occitanie', 'dre_pays_de_la_loire', 'dre_provence_alpes_cote_d_azur'].csv/_temporary/0/_temporary/attempt_202202040111283260659664240352794_0025_m_000000_25 (exists=false, cwd=file:/C:/Users/Antoine/Documents/spark_nrg-master)\r\n\tat org.apache.hadoop.fs.ChecksumFileSystem.create(ChecksumFileSystem.java:458)\r\n\tat org.apache.hadoop.fs.ChecksumFileSystem.create(ChecksumFileSystem.java:443)\r\n\tat org.apache.hadoop.fs.FileSystem.create(FileSystem.java:1118)\r\n\tat org.apache.hadoop.fs.FileSystem.create(FileSystem.java:1098)\r\n\tat org.apache.hadoop.fs.FileSystem.create(FileSystem.java:987)\r\n\tat org.apache.spark.sql.execution.datasources.CodecStreams$.createOutputStream(CodecStreams.scala:81)\r\n\tat org.apache.spark.sql.execution.datasources.CodecStreams$.createOutputStreamWriter(CodecStreams.scala:92)\r\n\tat org.apache.spark.sql.execution.datasources.csv.CsvOutputWriter.<init>(CsvOutputWriter.scala:38)\r\n\tat org.apache.spark.sql.execution.datasources.csv.CSVFileFormat$$anon$1.newInstance(CSVFileFormat.scala:84)\r\n\tat org.apache.spark.sql.execution.datasources.SingleDirectoryDataWriter.newOutputWriter(FileFormatDataWriter.scala:126)\r\n\tat org.apache.spark.sql.execution.datasources.SingleDirectoryDataWriter.<init>(FileFormatDataWriter.scala:111)\r\n\tat org.apache.spark.sql.execution.datasources.FileFormatWriter$.executeTask(FileFormatWriter.scala:269)\r\n\tat org.apache.spark.sql.execution.datasources.FileFormatWriter$.$anonfun$write$15(FileFormatWriter.scala:210)\r\n\tat org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:90)\r\n\tat org.apache.spark.scheduler.Task.run(Task.scala:131)\r\n\tat org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$3(Executor.scala:497)\r\n\tat org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:1439)\r\n\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:500)\r\n\tat java.util.concurrent.ThreadPoolExecutor.runWorker(Unknown Source)\r\n\tat java.util.concurrent.ThreadPoolExecutor$Worker.run(Unknown Source)\r\n\t... 1 more\r\n"
     ]
    }
   ],
   "source": [
    "saveDataFrame(df_production, f\"production_{fileName}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "saveDataFrame(df_consomation, f\"consomation_{fileName}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
